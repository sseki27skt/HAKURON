<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; 楽譜の機械可読化 – 楽譜記述の機械可読化による無形文化財の保護・継承・研究</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter5.html" rel="next">
<link href="./chapter3.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-707d8167ce6003fca903bfe2be84ab7f.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-89a64f88dd7f5cc3d225808ec1d4fc6b.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-d17b104d5b837865ffa01a6a0dcfcc41.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="site_libs/bootstrap/bootstrap-89a64f88dd7f5cc3d225808ec1d4fc6b.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://jtab.tardate.com/javascripts/jquery.js" type="text/javascript"></script>
<script src="https://jtab.tardate.com/javascripts/raphael.js" type="text/javascript"></script>
<script src="https://jtab.tardate.com/javascripts/jtab.js" type="text/javascript"></script>
<link type="text/css" rel="stylesheet" href="https://jtab.tardate.com/css/jtab-helper.css">
<script src="src/ruby_enabler2.js"></script>
<link rel="stylesheet" media="screen" href="https://fonts.googleapis.com/css2?family=Noto+Music&amp;family=Noto+Sans" type="text/css">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter4.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">楽譜の機械可読化</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./index.html" class="sidebar-logo-link">
      </a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">楽譜記述の機械可読化による無形文化財の保護・継承・研究</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter0.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">序</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">文化財保護と記録</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">音楽文化の記録としての譜</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">雅楽とその記録としての譜</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter4.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">楽譜の機械可読化</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">XMLを用いた雅楽譜の翻刻</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">結</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#音楽と情報技術の関わり" id="toc-音楽と情報技術の関わり" class="nav-link active" data-scroll-target="#音楽と情報技術の関わり"><span class="header-section-number">5.1</span> 音楽と情報技術の関わり</a>
  <ul class="collapse">
  <li><a href="#音楽情報のデジタル化に対する複数のアプローチ" id="toc-音楽情報のデジタル化に対する複数のアプローチ" class="nav-link" data-scroll-target="#音楽情報のデジタル化に対する複数のアプローチ"><span class="header-section-number">5.1.1</span> 音楽情報のデジタル化に対する複数のアプローチ</a></li>
  <li><a href="#機械可読な音楽情報記述の理想" id="toc-機械可読な音楽情報記述の理想" class="nav-link" data-scroll-target="#機械可読な音楽情報記述の理想"><span class="header-section-number">5.1.2</span> 機械可読な音楽情報記述の理想</a></li>
  </ul></li>
  <li><a href="#音楽情報の機械可読化に向けたアプローチ" id="toc-音楽情報の機械可読化に向けたアプローチ" class="nav-link" data-scroll-target="#音楽情報の機械可読化に向けたアプローチ"><span class="header-section-number">5.2</span> 音楽情報の機械可読化に向けたアプローチ</a>
  <ul class="collapse">
  <li><a href="#音楽情報を構成する領域" id="toc-音楽情報を構成する領域" class="nav-link" data-scroll-target="#音楽情報を構成する領域"><span class="header-section-number">5.2.1</span> 音楽情報を構成する領域</a></li>
  <li><a href="#smdlにおける音楽情報の四領域記述モデル" id="toc-smdlにおける音楽情報の四領域記述モデル" class="nav-link" data-scroll-target="#smdlにおける音楽情報の四領域記述モデル"><span class="header-section-number">5.2.2</span> SMDLにおける音楽情報の四領域記述モデル</a></li>
  <li><a href="#論理領域と視覚領域の分離が求められた背景" id="toc-論理領域と視覚領域の分離が求められた背景" class="nav-link" data-scroll-target="#論理領域と視覚領域の分離が求められた背景"><span class="header-section-number">5.2.3</span> 論理領域と視覚領域の分離が求められた背景</a></li>
  <li><a href="#音楽情報の記述に際して生じる資料の中心性に関する問題" id="toc-音楽情報の記述に際して生じる資料の中心性に関する問題" class="nav-link" data-scroll-target="#音楽情報の記述に際して生じる資料の中心性に関する問題"><span class="header-section-number">5.2.4</span> 音楽情報の記述に際して生じる資料の中心性に関する問題</a></li>
  <li><a href="#meiにおける四領域モデルの適用" id="toc-meiにおける四領域モデルの適用" class="nav-link" data-scroll-target="#meiにおける四領域モデルの適用"><span class="header-section-number">5.2.5</span> MEIにおける四領域モデルの適用</a></li>
  <li><a href="#音楽情報の重要性とmirの課題" id="toc-音楽情報の重要性とmirの課題" class="nav-link" data-scroll-target="#音楽情報の重要性とmirの課題"><span class="header-section-number">5.2.6</span> 音楽情報の重要性とMIRの課題</a></li>
  </ul></li>
  <li><a href="#小結" id="toc-小結" class="nav-link" data-scroll-target="#小結"><span class="header-section-number">5.3</span> 小結</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-MachineReadableSheetMusic" class="quarto-section-identifier"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">楽譜の機械可読化</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>本章では，先に整理してきた雅楽とその楽譜が持つ特徴を踏まえたデジタル雅楽譜の構築に向け，西洋音楽を中心として議論されてきた音楽情報の記述モデルの整理を行う． 音楽情報のデジタル化を目的として複数の側面から展開された先行事例を取り上げつつ，特に楽譜によって媒介される何らかの情報表現の記述を担うことを期待されているデータ記述形式に注目して，それぞれのデジタル楽譜に対するアプローチの違いを明らかにするとともに，西洋音楽とは異なる文脈に位置する雅楽における音楽文化の記録を目的とした情報記述モデルに必要な要素を明らかにしていく．</p>
<section id="音楽と情報技術の関わり" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="音楽と情報技術の関わり"><span class="header-section-number">5.1</span> 音楽と情報技術の関わり</h2>
<p>音楽を楽譜によって表現するということは，連続的に変化する音の響きを一定の規則，解像度に基づいて記号に置き換える行為であり，ある種の符号化とみなすことができる．もちろん，符号としての楽譜が持つ情報の粒度はその楽譜の文化によって異なるが，音楽という手に取ることも視覚的にとらえることもできない作品を何らかのルールに基づいて記述することで音楽的な知識や情報を保持させようとする姿勢は，デジタル技術を用いて音楽情報を記述しようとする本研究の姿勢にも通じている．また，音楽が音楽である以前に音ないし波であることによってもたらされる物理的な性質を通して音楽について検討するアプローチからも，音楽とコンピュータの近接性は描き出すことができる．</p>
<section id="音楽数計算機の交わり" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="音楽数計算機の交わり">音楽・数・計算機の交わり</h4>
<p>現在の音楽環境は制作，消費，研究に至るまで，あらゆる水準でコンピュータ，すなわち計算機の存在がなくてはならないが，そもそも，音楽と数の関わりは世界を数の調和で理解しようとした古代ギリシャ人の思想にまで遡ることができる．ピュタゴラス以降の音楽理論をまとめ，ヨーロッパに体系的な音楽理論をもたらしたboethius<span class="citation" data-cites="Boethius1867">(<a href="references.html#ref-Boethius1867" role="doc-biblioref">1867=2023</a>)</span>は，算術，音楽，幾何学，天文学を数学的四科として，それらの位置付けを以下のように示している．</p>
<blockquote class="blockquote">
<p>不動の〈<ruby>大きさ<rt>マグニチュド</rt></ruby>〉の考察は幾何学が保持し，動く〈大きさ〉の<ruby>学問<rt>スキエンティア</rt></ruby>は天文学が探究する．それ自体として存在する，隔てられた<ruby>〈量〉<rt>クゥアンティタス</rt></ruby>に関しては算術が権威であり，あるものに対して関連づけられた〈量〉に関しては音楽が<ruby>知識<rt>ペリティア</rt></ruby>を有することが認められている．</p>
<div style="text-align: right;">
<p><span class="citation" data-cites="Boethius1867">(<a href="references.html#ref-Boethius1867" role="doc-biblioref">Boethius 1867=2023, 156</a>)</span></p>
</div>
</blockquote>
<p>このように，古代ギリシャ人たちは「“関連づけられた/対比された量”つまり数比を素地として考察が展開される数学的学問の一分野」<span class="citation" data-cites="Boethius1867">(<a href="references.html#ref-Boethius1867" role="doc-biblioref">Boethius 1867=2023, 34</a>)</span>として音楽を位置付け，数学的な観点から音楽を探究していった <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>． 音楽を計算機で扱おうとする試みはコンピュータの歴史の中で比較的初期から存在するが，数学による理解が目指された音楽を計算機で扱うということはごく自然な流れだろう．また，音楽は科学技術の発展に伴う楽器の近代化，録音技術，放送技術など，様々な技術を取り込みながら発展してきており，コンピュータという新しい技術の導入にも積極的な土壌が形成されていたとも考えられる．</p>
<p>本章では，音楽が数，すなわち<ruby>計算<rt>コンピュート</rt></ruby>の対象であることを1つの起点として，コンピュータを用いた音楽研究の系譜をたどり，楽譜の機械可読化に向けたこれまでの研究とその課題を明らかにしたうえで，本研究における雅楽譜の符号化に向けたアプローチについて検討する．</p>
</section>
<section id="音楽情報のデジタル化に対する複数のアプローチ" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="音楽情報のデジタル化に対する複数のアプローチ"><span class="header-section-number">5.1.1</span> 音楽情報のデジタル化に対する複数のアプローチ</h3>
<p>音楽に関する資料には様々な形式のデータが含まれる．現在では，スキャンされた楽譜の画像データ，演奏を録音した波形データ，演奏家による楽器の操作を記録したMIDIデータなど，様々なデータを対象に研究が行われているが，機械可読に翻刻された楽譜データがその対象として取り扱われるようになったのは21世紀に入ってからのことである．ただし，“機械可読に翻刻された楽譜データ”と呼ばれうる対象の内部には，コンピュータを用いた楽譜情報の処理に対して異なる目的やコンセプトを掲げて展開された複数の成果が混在しており，デザイン方針の大きく異なるフォーマットがどれも“機械可読な楽譜データ”として扱われていることに注意する必要がある．</p>
<section id="演奏データの楽譜としての利用" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="演奏データの楽譜としての利用">演奏データの楽譜としての利用</h4>
<p>機械可読な楽譜データの事例としてまず挙げられるのが，MIDIを使用した楽曲データである．特にMIRの分野においてSymbolic Music Dataといった場合はその大半がMIDIであり，データの入出力や分析対象となるデータのフォーマットとして最も広く採用されている．例えば，機械学習用のデータセットであるSymbolic Music Data Version 1.0<span class="citation" data-cites="Walder2016">(<a href="references.html#ref-Walder2016" role="doc-biblioref">Walder 2016</a>)</span>や音楽の理解を目的とした大規模事前学習モデルであるMusicBERT<span class="citation" data-cites="Zeng2021">(<a href="references.html#ref-Zeng2021" role="doc-biblioref">Zeng et al. 2021</a>)</span>など，MIDIを使用して構築されたデータセットや学習モデルは挙げればきりがないほど数多く存在している．</p>
</section>
<section id="midiの目的と機能" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="midiの目的と機能">MIDIの目的と機能</h4>
<p>そもそも，演奏情報を記述するMIDIは異なるメーカーによって作られたシンセサイザー同士の連携を実現するために制定された．MIDI以前，70年代のシンセサイザー市場は各メーカーによる囲い込み戦略によってもたらされた独自規格の蔓延によって分断されており，一度あるメーカーの製品を買うと，互換性を保つために同じメーカーの製品で機材を買い続ける必要があった．このような状況は機材の汎用的な接続性を求めるユーザーから不評なだけでなく，シンセサイザー市場全体の成長を阻害する要因の1つとなっていた<span class="citation" data-cites="Chadabe2001">(<a href="references.html#ref-Chadabe2001" role="doc-biblioref">Chadabe 2001</a>)</span>．そこで，Rolandの創業者である梯郁太郎はシンセサイザー市場のさらなる成長のためには標準規格が必要不可欠と考え，Oberheim Electronics社，Sequential Circuits社とともに <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> MIDIの整備に乗り出す<span class="citation" data-cites="Chadabe1997">(<a href="references.html#ref-Chadabe1997" role="doc-biblioref">Chadabe 1997, 194</a>‐195)</span>．1982年には世界初のMIDI規格対応シンセサイザーであるPROPHET-600がSequential Circuits社から発売され，1983年のNAMM Show <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> ではPROPHET-600とRolandのJUPITER-6を用いたデモンストレーション <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> が行われ，MIDIが本格的に市場へと投入された．1983年にはMIDIとともにFM音源を搭載し，アマチュアからプロシーンまでの幅広いユーザーに受け入れられたYAMAHAのDX7など，音楽シーンを変えたシンセサイザーの名機も生み出され，電子楽器はデジタル時代 <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> へと突入していくことになる．</p>
</section>
<section id="midiにおける音楽情報記述のコンセプト" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="midiにおける音楽情報記述のコンセプト">MIDIにおける音楽情報記述のコンセプト</h4>
<p>MIDIは規格の標準化によるシンセサイザー市場の活性化を狙って採用された．したがって，その仕様は市場にとって馴染みのある楽器が持つ鍵盤のコンセプトを表現するものになっている．簡潔に言うと，MIDIは鍵盤を押して音を出したり止めたりする操作の表現をベースに設計されている <a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> ．わずか8ページの仕様書で定義された1982年当時のMIDIでは2台のシンセサイザーが送受信する最も基本的な命令しか定義されていなかったが，コンピュータの普及とともに非同期な打ち込み音源の制作にMIDIが応用されていく．これに対応する形で，音色のマッピングやチャンネルの割当規則，同時発音数などを規定したGMが制定され，演奏データを記述するための標準規格としてSMFが登場する．これによって，異なる楽器同士はもちろんのこと，シーケンサー，コンピュータ，ライティング・コントロール，ミキサーなどを相互に接続し，演奏情報を交換することが可能になった．現在ではライヴ・パフォーマンスに限らず，レコーディング・スタジオやビデオ制作現場，作曲などにも大きな影響を与えている．しかし，驚くべきことにその伝送に関わる基本的な仕様は1983年から現在まで全く変わっていない<span class="citation" data-cites="MIDIAssociation2015">(<a href="references.html#ref-MIDIAssociation2015" role="doc-biblioref">MIDI Association 2015</a>)</span>．</p>
<p>先にMIDIが楽譜データとして用いられうると指摘したが，実際にMIDIが伝送するのは演奏情報である．演奏情報とは演奏者が楽器をどのように演奏したか，具体的には，どの高さの音を，どれくらいの長さ，どれくらいの強さで演奏したのかという記録である <a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> ．これは自動ピアノに用いられるピアノロールのデジタル版と考えるとわかりやすい．演奏情報が記述されるSMFのトラック･チャンク <a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> には，音の始まりと終わりを指示するEvent Type <a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> に対して，Key Number(音高) <a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> ，Elapsed Time(経過時間) <a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> ，Delta Time(待機時間) <a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> ，Velocity(強さ) <a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> の4つの情報が1つのセットとして記述され，<code>Note on</code>と<code>Note off</code>を繰り返すことで旋律が記述されていく．MIDIは基本的に単純な旋律運動の伝達機能しか持っていないが，その高い汎用性によってあらゆる電子楽器をつなげる“電子楽器の楽譜”の標準規格として広く浸透し，楽譜知識の記述にも大きく影響を与えている．</p>
<div id="tbl-midivelocity" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-midivelocity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;5.1: MuseScore<span class="citation" data-cites="MuseScore2024">(<a href="references.html#ref-MuseScore2024" role="doc-biblioref">2024</a>)</span>が定義する強弱記号とベロシティの対応関係
</figcaption>
<div aria-describedby="tbl-midivelocity-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="{html}">
<style>
    /* テーブル全体のデザイン */
    table {
        width: 80%;
        border-collapse: collapse;
        text-align: center;
        margin: 2em auto;
        font-family: "Helvetica Neue", Arial, "Hiragino Kaku Gothic ProN", "Hiragino Sans", Meiryo, sans-serif;
    }

    /* ヘッダーのデザイン：二重線で本文と明確に区別 */
    th {
        padding: 12px 15px;
        font-weight: bold;
        border-bottom: 3px double #333;
    }

    /* セルのデザイン */
    td {
        padding: 12px 15px;
    }

    /* マウスホバー時のハイライト：控えめな色で */
    tbody tr:hover {
        background-color: #f5f5f5;
    }
</style>
<table>
<thead>
<tr>
<th scope="col">
強弱記号
</th>
<th scope="col">
ベロシティ
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<span class="math inline">\(\mathit{ffff}\)</span>, <span class="math inline">\(\mathit{fffff}\)</span>, <span class="math inline">\(\mathit{ffffff}\)</span>
</td>
<td>
127
</td>
</tr>
<tr>
<td>
<span class="math inline">\(\mathit{fff}\)</span>
</td>
<td>
126
</td>
</tr>
<tr>
<td>
<span class="math inline">\(\mathit{ff}\)</span>
</td>
<td>
112
</td>
</tr>
<tr>
<td>
<span class="math inline">\(\mathit{f}\)</span>
</td>
<td>
96
</td>
</tr>
<tr>
<td>
<span class="math inline">\(\mathit{mf}\)</span>
</td>
<td>
80
</td>
</tr>
<tr>
<td>
<span class="math inline">\(\mathit{mp}\)</span>
</td>
<td>
64
</td>
</tr>
<tr>
<td>
<span class="math inline">\(\mathit{p}\)</span>
</td>
<td>
48
</td>
</tr>
<tr>
<td>
<span class="math inline">\(\mathit{pp}\)</span>
</td>
<td>
32
</td>
</tr>
<tr>
<td>
<span class="math inline">\(\mathit{ppp}\)</span>
</td>
<td>
16
</td>
</tr>
<tr>
<td>
<span class="math inline">\(\mathit{pppp}\)</span>
</td>
<td>
10
</td>
</tr>
<tr>
<td>
<span class="math inline">\(\mathit{ppppp}\)</span>
</td>
<td>
5
</td>
</tr>
<tr>
<td>
<span class="math inline">\(\mathit{pppppp}\)</span>
</td>
<td>
1
</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
<p>このように，MIDIが持つ本来の機能からすると，それを楽譜として用いることは本来の目的から外れた利用法であるが，純粋な旋律運動だけを検討対象とする場合など，楽譜に書かれた情報の一部を抽出して利用する場合には有用なフォーマットとして機能しうる．実際，MuseScore，Finale，Sibeliusなどの楽譜制作ソフトウェアはMIDIファイルのインポート/エクスポートに対応しており，楽譜の記述からMIDIを作成することは非常に容易である．一方，MIDIから印刷楽譜を作成しようとする場合，MIDIの持っていない印刷レイアウトに関する情報を補う必要があり，その方法はコンバータに委ねられている．したがって，仮に楽譜制作ソフトウェアを使用して作成した五線譜からエクスポートされたMIDIであったとして，そこから元の楽譜を再現できる保証はない．また，楽譜に書かれている強弱記号などはMIDIで直接扱うことはできないため，何らかの読み替えが行われる場合もある．例えば，MuseScoreの場合は強弱記号とベロシティの対応を@tbl-midivelocity のように定義している．また，楽譜中に強弱を指示する記号が登場しない場合，すべてのMIDIイベントに対するベロシティは一律に80に設定される．このような定義はMIDIをエクスポートするうえで必須の要素であるベロシティを補完するために必要な情報ではあるが，仮に楽譜制作ソフトウェアを用いて既存の五線譜に基づくMIDIを作成し，それを楽譜データとして用いる場合，本来楽譜においては未定義の情報をデータ作成のために一部書き加えてしまっているということになる．</p>
</section>
<section id="楽譜記述のデジタル化に向けた最初期の事例" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="楽譜記述のデジタル化に向けた最初期の事例">楽譜記述のデジタル化に向けた最初期の事例</h4>
<p>このような演奏データが楽曲の構造を捉えた楽譜的なデータとして扱われる一方，楽譜制作そのもののデジタル化によってもたらされるデータも“機械可読な楽譜翻刻された楽譜データ”として扱われている．活版印刷の応用からはじまった楽譜印刷は長らく活版印刷，エングレービング，リトグラフの3つの印刷技術が共存する状態にあった．しかし，Erickson<span class="citation" data-cites="Erickson1975">(<a href="references.html#ref-Erickson1975" role="doc-biblioref">1975</a>)</span>によると，高品質な音楽制作に求められる技術や技能を持つ職人の減少に伴って1970年代初頭にかけて楽譜の価格が上昇し続け，それが楽譜の価格上昇や新譜及び古典的な作品の新版の発売を遅らせる原因になっていた．Erickson<span class="citation" data-cites="Erickson1977">(<a href="references.html#ref-Erickson1977" role="doc-biblioref">1977</a>)</span>は，このような楽譜価格の高騰を受けて，指揮者のStefan Bauer-Mengelberg（1927–1996）が1960年代初頭には既にコンピュータを用いた楽譜印刷の可能性を模索し始めていたことを指摘している．</p>
<p>IBMの数学者でありながら，Leonard Bernstein（1918–1990）の下でニューヨーク・フィルハーモニックの副指揮者を務めた経験のあるStefan Bauer-MengelbergはMelvin Ferentz <a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> とともに1961年にDARMS プロジェクトを立ち上げた<span class="citation" data-cites="Erickson1975">(<a href="references.html#ref-Erickson1975" role="doc-biblioref">R. F. Erickson 1975</a>)</span>． このプロジェクトではニューヨーク・フィルハーモニックで用いる楽譜の作成を容易にするという目的を掲げたプロジェクトであり，作曲ではなくあくまでも楽譜の作成における生産性を向上するという点で，作曲や演奏を目的としていたツールとは一線を画している<span class="citation" data-cites="CenterforComputerAssistedResearchintheHumanities2022">(<a href="references.html#ref-CenterforComputerAssistedResearchintheHumanities2022" role="doc-biblioref">Center for Computer Assisted Research in the Humanities 2022</a>)</span>．</p>
<p>DARMSは楽譜をそのオリジナルと同等な情報量を持った機械可読な形式に“翻訳”する手段であり，“解釈”は伴わないとされる<span class="citation" data-cites="Erickson1977">(<a href="references.html#ref-Erickson1977" role="doc-biblioref">R. Erickson 1977</a>)</span>．パンチカードで情報入力を行っていた1960年代のコンピュータで処理を行うため，その記述は極めてシンプルでありながら厳密な規則性を持っており，印刷楽譜とDARMSの記述の間には基本的に1対1の対応が実現されていた．また，DARMSが特徴的なのは，もともと楽譜の作成を目的としてスタートしたプロジェクトであるにもかかわらず，楽譜が持つ視覚的な構造を再現することではなく楽譜の理論的な側面を正確に記述することを優先している点である．その例として，Erickson<span class="citation" data-cites="Erickson1977">(<a href="references.html#ref-Erickson1977" role="doc-biblioref">1977</a>)</span>は全休符の記述を挙げている．五線譜に配置された音符や休符は基本的に記述された位置からその効力を発揮するが，全休符は一般的に小節の中央に配置することが記譜上の慣習になっている．つまり，全音符は“記述された位置からその効力を発揮する”という規則に当てはまらない例外である．人間が読む楽譜の再現を優先するのであれば，全休符を小節の中央に配置するという例外を認めるのが自然だが，DARMSはこのような例外を認めず，楽譜の視覚的な側面における再現性を妥協してまで，すべての音符及び休符はその効力を発揮する位置に記述するという規則を徹底している．</p>
<p>このような，厳密な規則に基づいた楽譜記述の手法は楽譜によって表現されたデータをコンピュータで分析することを目指すコミュニティから注目を集め，楽譜を作成するという目的を超えて，楽譜のデータ化という新たな可能性を提示するものとして受け入れられた．また，DARMSはMichael Kasslerが開発したIML-MIRとJerome Wenkerが開発したMustranをつなぐインターフェイスとして用いられることも想定されていた<span class="citation" data-cites="Erickson1977">(<a href="references.html#ref-Erickson1977" role="doc-biblioref">R. Erickson 1977</a>)</span>．</p>
</section>
<section id="darmsの記述構造" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="darmsの記述構造">DARMSの記述構造</h4>
<p>DARMSの記述は<code>!</code>から始まる複数のグローバル指定子で始まる．<a href="#fig-no24" class="quarto-xref">Figure&nbsp;<span>5.1</span></a> の場合は，楽譜が含む楽器の数（<code>!In</code>），音部記号（<code>!G</code>，<code>!F</code>，<code>!C</code>），拍子（<code>!Mn:n</code>）が含まれる <a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> ．続いて，各楽器の楽譜が音符の位置（<code>1</code>，<code>2</code>，<code>3</code>など）と音符の長さ（<code>Q</code>(四分音符)，<code>H</code>（二分音符），<code>W</code>（全音符）など）の組み合わせで記述される．音高を記述する方法は@fig-no25 のように，五線譜の線と間を順に数字で示す方法を採用している．同じ位置に符頭が配置されていたとしても，実際の音の高さは音部記号によって変化するが，五線の線と間を楽譜記述に採用することで，音の高さを正確に読み取ることができなくても，楽譜の入力ができる．これは，「楽譜が読めない人でも楽譜を作成可能にする」というDARMSのコンセプトを実現するための工夫である．</p>
<div id="fig-no24" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-no24-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode default code-with-copy"><code class="sourceCode default"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>!I1 !G !K !M2:4 -1Q Q / 3Q Q / 4Q Q / 3H</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-no24-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.1: DARMSを用いた『きらきら星』冒頭の記述
</figcaption>
</figure>
</div>
<div id="fig-no25" class="lightbox quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-no25-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/darms.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;5.2: DARMSにおける符頭位置の記述"><img src="img/darms.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-no25-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.2: DARMSにおける符頭位置の記述
</figcaption>
</figure>
</div>
</section>
<section id="楽譜組版のデジタル化に向けた取り組み" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="楽譜組版のデジタル化に向けた取り組み">楽譜組版のデジタル化に向けた取り組み</h4>
<p>一方，コンピュータを用いた楽譜組版技術の発展における最初期の成功例としてはStanford Artificial Intelligence Laboratory <a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>に所属していたLeland Smithが開発し，1987年にリリースされたSCOREが挙げられる．SCOREはGUIによる入力には対応していなかったが，複雑な楽譜の再現性，テキストの柔軟な取り扱い，パートの適切な抽出，自由なページレイアウトなど，機能面では非常に高い水準を誇っており，Copyist III，MusicPrinter Plus，Personal Composerなど，同時代の楽譜制作ソフトウェアを機能面では凌駕していた<span class="citation" data-cites="Bowles1990">(<a href="references.html#ref-Bowles1990" role="doc-biblioref">Bowles 1990, 679</a>)</span>．1988年にSchott Musicがいち早く導入したのを皮切りに，Ricordi，Petersなどのクラシック楽譜出版社はもちろんのこと，Hal Leonardなどポピュラー音楽楽譜出版社にも採用され，商業楽譜出版のワークフローの中に本格的に取り込まれたはじめてのデジタル楽譜組版ソフトウェアである<span class="citation" data-cites="SanAndreasEnterprises2003">(<a href="references.html#ref-SanAndreasEnterprises2003" role="doc-biblioref">San Andreas Enterprises 2003, Selfridge–Field2014</a>, p.)</span>．しかし，その柔軟性ゆえに非常に学習コストが高く，現在の私たちが想像するような個人向け楽譜制作ソフトウェアとは異なり，完全プロユースを前提とした設計になっていた．</p>
<p>一方，GUIを搭載した楽譜制作ソフトウェアとしては，現在でも高いシェアを誇るFinaleが1988年に，Sibeliusが1993年にリリースされる．これらのソフトウェアはGUIによる楽譜入力が可能であり，一般消費者向けの商品として広く普及している．しかしながら，Fujinaga<span class="citation" data-cites="Fujinaga2004">(<a href="references.html#ref-Fujinaga2004" role="doc-biblioref">2004</a>)</span>が指摘する通り，楽譜制作ソフトウェアが扱うデータフォーマットには長らく標準が存在しなかった．Sunhawk，MusicNotes，Sibelius，Noteheadsなどの楽譜データ販売サイトは機械可読な形式で表現された楽譜の持つ再生機能を1つの付加価値として楽譜データの販売を行ってきたが，それはMIDI以前のシンセサイザー市場同様，各楽譜制作ソフトウェアが独自のフォーマットを使用することによって消費者の囲い込みを狙った結果，販売元が提供する楽譜閲覧ソフトでしか閲覧，再生，印刷ができず，デジタル楽譜市場そのものが分断される状況が生じていた<span class="citation" data-cites="Good2001">(<a href="references.html#ref-Good2001" role="doc-biblioref">Good 2001</a>)</span>．</p>
</section>
<section id="musicxmlの登場" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="musicxmlの登場">MusicXMLの登場</h4>
<p>このような状況を解消するためのファイル交換フォーマットとして開発され，2001年に策定されたのがMusicXMLである．SAPに勤めていた開発者のMichael GoodはXML形式のファイルを使用して情報のやり取りをしている自社のシステムを楽譜制作ソフトウェアに応用するアイディアを思いつき，Recordare社を立ち上げてMusicXMLの開発を進めた<span class="citation" data-cites="Tschiggfrie2020">(<a href="references.html#ref-Tschiggfrie2020" role="doc-biblioref">Tschiggfrie 2020</a>)</span>．MIDIによるファイルフォーマットの標準化が電子楽器業界に大きな発展をもたらした前例を踏まえ，MusicXMLは独自の楽譜記述フォーマットをゼロから完成させるのではなく，互換性のない既存の楽譜フォーマットをつなぐための交換用フォーマットと，それを用いた楽譜ファイルコンバータの開発を並行して進める形で行われた．最初期のプロトタイプでは主に音楽情報検索の分野で使われてきたMuseDataとHumdrumを参照しながら，NIFF，MuseData，MIDIの3種類の楽譜フォーマットを仲介するフォーマットが整備され，NIFFで読み込んだファイルからMuseDataファイルを生成し，そこからMIDIデータを得るコンバータがあわせて開発された．MakeMusicが開発し有料楽譜制作ソフトとしてすでに普及していたFinaleやWindowsで動作する代表的な楽譜OCRソフトウェアであったSharpEye Music Scanningがサポートを開始したことで，MusicXMLは楽譜共有フォーマットとして急速に普及した<span class="citation" data-cites="Good2001 Tschiggfrie2020">(<a href="references.html#ref-Good2001" role="doc-biblioref">Good 2001</a>; <a href="references.html#ref-Tschiggfrie2020" role="doc-biblioref">Tschiggfrie 2020</a>)</span>．MusicXMLの開発はW3CのMusic Notation Community Groupに引き継がれ，現在でも継続してメンテナンスされている．</p>
</section>
<section id="musicxmlのコンセプト" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="musicxmlのコンセプト">MusicXMLのコンセプト</h4>
<p>MusicXMLはGUIを搭載した楽譜制作ソフトウェア間のデータ共有を目的として開発がスタートしており，楽譜が持つ視覚的な構成の再現性を特に優先している．楽譜制作ソフトウェアで作成された楽譜は最終的に印刷され，人間が読み取ることが前提だからである．仮に，楽譜ファイルに記録されている音楽の構造が論理的に間違っていたとしても，楽譜制作ソフトウェア上で人間に正しく読まれる形式で表示されてさえいれば楽譜としての機能に問題はない．究極的には画像ファイルなど，楽譜用のファイルフォーマットではなくても，人間にとって楽譜として読み取ることができる像が最終的に得られれば印刷楽譜データに対して求められる要求を満たすことができる．</p>
<p>ただし，既存の楽譜から読み取られた音楽構造の記述を目的としたフォーマットとして用いる際には，音楽構造の論理的な記述に対する曖昧さを許容することで印刷物として出力される結果の視覚的な再現性を維持しようとするMusicXMLの設計思想が不利に働くケースも考えられる．これは，楽譜制作ソフトウェアの多くがGUIを搭載したWYSIWYG <a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> な編集作業環境を提供しており，楽譜としての視覚的な正しさがその背後に存在するデータとしての正しさを必ずしも反映しているとは限らないためである． 例えば，Open Goldberg Variationsやその後継プロジェクトとして立ち上げられたOpen Well-Tempered Clavier <a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> の活動を引き継ぐOpenScoreプロジェクトは，著作権の切れた楽譜を網羅的に収集する電子図書館を目指すIMSLP，オープンでフリーな楽譜制作ソフトウェアと楽譜データ共有プラットフォームを提供するMuseScoreの協力のもと，IMSLPに収集された全てパブリックドメイン楽譜をMuseScoreでデジタル翻刻された「インタラクティブ･スコア」にすることを究極的な目標として掲げてスタートしたが，GUIを搭載した楽譜制作ソフトウェアの利用に伴う課題を克服することができずにプロジェクト自体の方向を大きく変えざるを得ない状況に直面した<span class="citation" data-cites="OpenScore2018a">(<a href="references.html#ref-OpenScore2018a" role="doc-biblioref">OpenScore 2018, Jonas2017a</a>)</span>．IMSLPにアップされているの楽譜を視覚的に正しく再現することに関して，OpenScoreプロジェクトに参加した翻刻ボランティアやそのレビューに関わったユーザーは非常に注意深く作業に取り組んでおり，単純な誤植はレビューの段階で素早く発見された．つまり，「間違っているように見える」箇所を探す段階においては非常に高いパフォーマンスを発揮した． 一方，構造的に正しいデータを構築していくためには，五線譜としてレンダリングされた状態では見つけることが難しい意味的な間違いを探し出して修正していく必要がある． 楽譜翻刻ボランティアやレビュアーが楽譜の視覚的な間違いに敏感である一方，紙面上に配置された記号から人間が読み取った意味がデジタル楽譜の記述として正しく反映されているかどうかは，GUIを搭載したWYSIWYGな楽譜編集環境をユーザーに提供している状況では視覚的に判断することが難しく，一見「正しく見える」楽譜が「構造的には間違っている」ということをユーザーに理解してもらうのは困難であった<span class="citation" data-cites="OpenScore2018a">(<a href="references.html#ref-OpenScore2018a" role="doc-biblioref">OpenScore 2018</a>, p.)</span>．</p>
<p>OpenScoreプロジェクトの失敗は，デジタル楽譜という言葉から連想される2つのコンセプトが十分に整理されないまま，プロジェクトの主導者と参加者の間に認識の相違が生じてしまったことが1つの理由として挙げられるだろう．プロジェクト主宰者はOpenScoreプロジェクトを通して楽譜から読み取られる音楽の構造が正確に反映された楽譜データの構築を目指していた．一方，翻刻に参加したボランティアにとっては元のと同じレイアウトを持った編集・再生が可能な楽譜を最終的な成果物として想定しており，その背後に存在するデータの正確性に関心が持たれていなかったと考えられる．また，実際に楽譜を翻刻してみると，例えば強弱記号1つとっても，あいまいな位置にある <span class="math inline">\(\mathit{Crescendo}\)</span> や <span class="math inline">\(\mathit{Decrescendo}\)</span> の開始・終了位置をどの音符と関連づけるか，複数声部で構成された旋律をどのように分割するか，といった問題は単純な書き写しのように解決できるものではなく，資料批判的な視点も求められる作業である．それらをボランティアの共同作業によって解決するというプロジェクト自体の設計にも多少無理があったと言わざるを得ない．</p>
<p>このように，印刷物としての楽譜が持つレイアウトを可能な限り維持することを志向する楽譜制作ソフトウェアでの利用を想定したデータ形式は，読譜リテラシーを持つユーザーにとって使いやすいインターフェイスであるがゆえに，楽譜の持つ音楽的な構造がデータとして必ずしも正確に反映されているとは限らず，またその間違いが視覚的に判別しにくいという問題がある．演奏データであるMIDIと比べれば，楽譜の記述内容をより深く反映しているかもしれないが，その内部に保持している構造化データが翻刻の対象となった楽譜から読み取れる情報を適切に反映しているとは限らない．</p>
<p>また，そもそも楽譜“交換”フォーマットであるMusicXMLが独立した楽譜フォーマットとして利用されている状況は，本来想定している用途とは異なる使用方法である．ソフトウェアを跨いだ楽譜データの共有が可能ということは，MusicXMLがあらゆる楽譜制作ソフトウェアで利用されるデジタル楽譜フォーマットになっていてもおかしくないはずだが，実際にはそうはなっておらず，楽譜制作ソフトウェアがそれぞれ独自のファイルフォーマットを用いている状況自体は現在でも変わらない．また，視覚的な再現性を比較的重視しているMusicXMLも最終的なレンダリングは各楽譜制作ソフトウェアのレイアウトエンジンに依存しているため，MusicXMLファイルを読み込んだときにその楽譜がどのように表示されるかは，そのファイルを読み込むソフトウェアによって異なる．また，Rettinghaus<span class="citation" data-cites="Rettinghaus2023">(<a href="references.html#ref-Rettinghaus2023" role="doc-biblioref">2023</a>)</span>は楽譜制作ソフトウェアによってサポートしている出力要素に違いがあるため，同じ見た目になるように組版された楽譜データであっても，ソフトウェアによって異なるMusicXMLがエクスポートされることを報告している．もちろん，音符，小節，音部記号，調号といった基本的な構成要素はどのソフトウェアもサポートしているが，上下の譜表を横断するアルペジオや装飾音，文字による演奏指示の扱いはそれぞれのソフトウェアによって異なり，場合によってはエクスポートに対応していない楽譜記述も存在する <a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a>． このことからも，MusicXMLはあくまでも楽譜“交換”フォーマットであり，一度入力した楽譜の基本的な構成を共有することは可能だが，最終的に楽譜としてのレイアウトを整えるためには，各楽譜制作ソフトウェアの独自のフォーマットを用いて楽譜を再編集することが前提となっていることがわかる．また，MusicXMLが登場した2001年当時と比べると，近年の楽譜制作ソフトウェアは非常に多くの機能を提供しており，その中にはそもそもMusicXMLが対応していない機能もある<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a>．したがって，楽譜制作ソフトウェア間のファイル共有フォーマットとは言いながらも，実態としては「楽譜制作ソフトウェアが提供する機能の基本的な楽譜構成要素を編集可能な状態で共有する」という点においてのみ，楽譜共有ファイルフォーマットとして機能している状態にある．</p>
</section>
<section id="目録情報としての楽譜データ" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="目録情報としての楽譜データ">目録情報としての楽譜データ</h4>
<p>ここまでに取り上げた2つのアプローチに加え，学術的な側面からは音楽図書館やアーカイブに所蔵されている楽譜資料のデジタル目録の一部として楽譜データを取り入れようとする活動も展開されている．</p>
<p>最も大規模な例として挙げられるのが，RISMが提供する音楽資料目録である．1952年に設立されたRISMは世界中の現存する音楽資料を包括的に記録することを目的とする国際非営利組織であり，図書館，公文書館，教会，学校，個人コレクションなどが所蔵している写本，印刷楽譜，音楽理論書，歌詞カードなどを対象に150万件以上の資料が登録されている世界最大規模の音楽資料目録の1つである．</p>
<p>音楽資料の目録作成に際しては，作曲者，作品のタイトル，出版年，出版社，ジャンル，楽器編成，作品番号，ページ数，大きさ，所蔵者情報など，楽譜資料が持つ多様な情報が記録されるが，その一部にインキピットと呼ばれる項目が存在する．インキピットはタイトルのない文書や作品の冒頭部分を見出しとして使用する習慣であり，詩歌，聖書の一節，法律など，音楽以外の分野でも用いられている <a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a>．複数楽章から構成されるソナタや交響曲の楽譜で，目次に各楽章のインキピットが記されている場合もある．</p>
</section>
<section id="デジタル音楽資料目録のためのデジタル楽譜表現" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="デジタル音楽資料目録のためのデジタル楽譜表現">デジタル音楽資料目録のためのデジタル楽譜表現</h4>
<p>RISMの目録に登録されているインキピットをデジタル化するために，IAMLとRISMはPAEと呼ばれる楽譜記述用の記述規則を1964年に策定し，現在に至るまで維持管理を続けている．RISMではタイトルの有無にかかわらずデータベースに登録された楽曲にインキピットを付与しており，その数は170万件以上におよぶ． 音楽資料のメタデータ記述形式の1つであり，音楽図書館における情報交換フォーマットであるPAEは2004年には目録データ交換用フォーマットであるMARCの一部としても採用されている<span class="citation" data-cites="LibraryofCongress2024">(<a href="references.html#ref-LibraryofCongress2024" role="doc-biblioref">Library of Congress and Network Development and MARC Standards Office 2024</a>)</span>．</p>
<p><a href="#fig-no26" class="quarto-xref">Figure&nbsp;<span>5.3</span></a> に示すように，目録データ作成の習慣に合わせた記述形式であるPAEは非常にシンプルな構造で構成されている．アーカイブズ資料としての楽譜を主な記述対象とすることから，図<span class="math inline">\(\ref{fig:brevis}\)</span>に示すような古い楽譜の記述にまでサポートが及んでおり，シンプルながら目録作成という特殊なニーズに対応した楽譜記述フォーマットになっている． 器楽作品の場合は最初の数小節を，声楽作品の場合は冒頭の歌詞を使用し，少なくとも3小節，10音符以上を含めることが推奨されている．複数の声部がある場合などに楽曲のどこを入力するかは入力作業者に委ねられているが，音楽的に意味のある箇所を選択することが求められており，主には主旋律を担当することの多いヴァイオリン，もしくは最高音部を担当する楽器のパートが選択されることが多い．</p>
<div id="fig-no26" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-no26-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode default code-with-copy"><code class="sourceCode default"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>%G-2 @4/4 '4CCGG/AA2G/4FFEE/DD2C</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-no26-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.3: PAEによる『きらきら星』の譜例
</figcaption>
</figure>
</div>
<p>目録の一部であるPAEにとっては検索性が非常に重要である．RISMのカタログではincipit search機能が実装されており，入力した楽曲のフレーズをクエリとして，RISMのデータベースから類似したフレーズを持つ楽曲を検索することができる．音楽作品の中には，作品タイトルや作曲家，制作年などによって一意に特定できないものも多いため，インキピットは楽曲の検索として有効なアプローチである．ただし，RISMが提供するインキピットデータベースは楽譜資料に現れた状態をそのまま記述したものであり，同じ楽曲同士でインキピットを正規化することは一切されていない．したがって，同じタイトルを持つ楽曲であったとしても，楽譜冒頭の旋律が異なる場合には，必要な楽譜資料が検索にヒットしない場合もある．RISMとしては楽譜1点を1つの単位として扱っており，楽譜上に現れる旋律が異なるのであれば，インキピットもそれに従う方針をとっているが，コンピュータで処理をする場合，ある種の異読や表記ゆれは正規化した上で検索結果をユーザーに提供した方が高いユーザービリティが期待できる場合もある．しかしながら，現状のPAEはそのような異読や異版によって生じる表現の僅かな違いを正規化して表現する方法は提供していないため，旋律を用いた目録データの検索には限界があることも事実である．</p>
</section>
</section>
<section id="機械可読な音楽情報記述の理想" class="level3" data-number="5.1.2">
<h3 data-number="5.1.2" class="anchored" data-anchor-id="機械可読な音楽情報記述の理想"><span class="header-section-number">5.1.2</span> 機械可読な音楽情報記述の理想</h3>
<p>このように，機械可読な楽譜データとして扱われうるデータは本来その情報粒度は様々であり，演奏データであるMIDIは基本的な音の長さや高さといった情報を持つ一方で，印刷物としての楽譜に現れる視覚的な要素についてはほぼサポートしていない．一方，楽譜制作ソフトウェアでの利用を想定したMusicXMLは楽譜の視覚的な再現性をより重視しているが，その楽譜の持つ音楽的な構造が常に正確に記述されているとは限らず，時にGUIを搭載する楽譜制作ソフトウェアを使用するが故にエラーを発見することが難しいという問題を抱えている．</p>
<p>Müller<span class="citation" data-cites="Müller2021">(<a href="references.html#ref-Müller2021" role="doc-biblioref">2021</a>)</span>はMIRが音楽情報として扱いうる表現として，印刷楽譜，シンボリックデータ，オーディオデータの3つを挙げているが，同時にこれらの境界は曖昧であると指摘している．つまり，これらの表現をどのように扱うかによって，そこから引き出される音楽情報は変化し，それに応じて選択すべきフォーマットも変化していく．また，一度何らかの形で機械可読な状態に置かれた音楽情報は，コンバータによってその姿を変化させることができる．本来は楽譜制作ソフトウェア間のデータ共有フォーマットであるMusicXMLが持つ音楽の構造に関する記述を使用してMIDIデータを作成し，さらにそのMIDIデータを用いて音声ファイルを生成することもできてしまう．楽譜の構造を西洋音楽が定義する音高や音価の表現によって記述するMusicXMLから得られたデータのMIDIへの変換は基本的に不可逆的ではあるものの，MusicXMLに記述された情報の一部を異なる表現に置き換えたという点においては，その内部には記述の対応関係が保持されており，両者ともに西洋音楽が構築してきた音楽理論や概念を前提とした記述構造を持つという点において，全く共通する部分がないわけではない <a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>． 一方，MIDIを音声ファイルに変換する水準にまで至ると，情報を抽出するだけにとどまらず，符号化データから波形データへと，データ記述の基盤そのものが大きく変化しており，その変換に際しては音響生成に必要な情報を大幅に補っている．そのなかでも，最も大きいのは音色に関する情報であろう．MIDIはその規格内部で楽器の指定ができるが<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a>，同じピアノでもヤマハと河合で音色が異なるように，実際にどのような音が鳴るかは音源ソフトに完全に依存している．</p>
<p>変換によって1つのデータを複数の目的で利用することは，ある情報をデジタルデータとして扱うことによる大きなメリットの1つである．しかしながら，殊に音楽情報については，それ自体の表現形態も，その表現を捉えるために使用されるデータフォーマットも多岐にわたっており，それぞれが持つ記述内容についても異なる可能性がある．音楽データの変換を実現するために背後で捨象されている，もしくは補われている情報に細心の注意を払う必要がある．</p>
<section id="人文系学問が信頼可能なデジタル楽譜" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="人文系学問が信頼可能なデジタル楽譜">人文系学問が信頼可能なデジタル楽譜</h4>
<p>このようなデータの変換に伴う内容の変化は，データそのものが持つ情報の信頼性に影響を及ぼす．特に音楽学において重視されてきた揺るぎなき楽譜の真正性の概念は，このような内部記述の書き換えを伴う変換に対して非常に脆弱であろう．</p>
<p>また，音楽学をはじめとする伝統的な人文系学問において，資料に対する正確性，正当性，信頼性は，その学問を成り立たせる上で欠かすことのできない要素であり，彼らにとって重要なのは特定のスキーマに基づいた“綺麗なデータ”を作成することではなく，仮に構造化の水準が混在していたり，構造そのものが複雑であったりしたとしても，資料全体の信頼を担保する確かな情報が十分に盛り込まれていることである．そのような観点で見ると，MIDIやMusicXMLのような対象となる利用シチュエーションが限定されたデータフォーマットは人文系学問が必要とするデータ記述構造としては，その内部に記述する情報が不足していると言わざるを得ない．楽譜を軸に展開されてきたこれまでの音楽研究にデジタル技術を導入するためには，音楽を記述するための言語として楽譜を扱ってきたこれまでの姿勢を反映したデータ形成を目指しながら，デジタル技術の導入による新たな可能性を探求していくことが求められる．</p>
<p>そのための1つの手段として現在構築されつつあるのがMEIである．MEIはMARCやFRBRなど既存のメタデータ規格とともに，人文学資料の適切なデジタル化に向けたガイドラインの策定を行うTEIから大きな影響を受けており，楽譜の視覚的情報だけではなく，より詳細で多層的な音楽資料の情報記述を可能にしている．</p>
<p>MEIも楽譜の構造と紙面構成をXML形式で表現する点においてはMusicXMLと同じ機能を有しているが，それ以上に楽譜とその知的コンテンツに関する情報を構造的かつ体系的に符号化することに注力している．いわば人文学が用いる電子テキストの構造化を目指すTEIの音楽版であり，TEIが積み重ねてきた数多くの成果を取り込み，多くの共通した特徴や開発手法を採用している．</p>
<p>Music Encoding Initiative<span class="citation" data-cites="MusicEncodingInitiative2024a">(<a href="references.html#ref-MusicEncodingInitiative2024a" role="doc-biblioref">2024</a>)</span>は音楽の歴史的研究に従事する研究コミュニティが以下の要件を満たす符号化システムを必要としているとしている．</p>
<blockquote class="blockquote">
<ul>
<li>五線譜にとどまらず，ネウマ譜や定量記譜法，モーダル記譜法，タブラチュアなど，多様な西洋記譜法が持つ意味的・論理的な複雑性を表現できる</li>
<li>ファクシミリ版，原典版，実用版など，楽譜の形態に関わらず共通して認められる表記上の特徴を表現できる</li>
<li>学術コミュニティによって管理されるパブリックでオープン標準な規格である</li>
<li>特定のプラットフォームに依存せずオープン標準な規格に基づいている</li>
<li>学術的な分析をサポートし，デジタルと印刷の両方で柔軟な表現ができる</li>
</ul>
</blockquote>
<p>これらの要求に対して，MEIは意味的に豊かな楽譜表記モデルの作成を目指し，以下のような目標を掲げている．</p>
<blockquote class="blockquote">
<ul>
<li>一般的な西洋音楽に対応するが，それに限定されない．</li>
<li>研究コミュニティが学術的な用途のために設計するが，その他の用途を排除しない．</li>
<li>伝統的なファクシミリ版，校訂版，演奏版楽譜が持つ標準的な用途に対応する．</li>
<li>研究者の目的に応じて使い分けられるモジュール構造を持つ．</li>
<li>オープン標準<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a>を基盤とし，特定のプラットフォームに依存しない．</li>
<li>楽譜の国際的な包括的かつ恒久的なアーカイブの開発を可能にし，エディション研究，楽曲分析，演奏研究，その他の研究の基礎となる．</li>
</ul>
</blockquote>
<p>MusicXMLが五線譜の記述をベースにしているのに対して，上記のような音楽学コミュニティからもたらされる要求に応えることを目指すMEIは楽譜の定義そのものをより柔軟に捉えている．MEIが記述の対象とする音楽的文書には当然ながら楽譜，すなわち 「実際に響いた音やイメージされた音の記録として，また演奏者への一連のサインとして，楽音を視覚的に記号化したもの」<span class="citation" data-cites="Bent2001">(<a href="references.html#ref-Bent2001" role="doc-biblioref">Bent et al. 2001</a>)</span>が含まれることは当然のこと，非視覚的なサインである点字譜や楽譜資料そのものが持つメタデータ，音源資料などを含む音楽関連資料全般を包括的に扱うことを可能にしている<span class="citation" data-cites="MusicEncodingInitiative2023a">(<a href="references.html#ref-MusicEncodingInitiative2023a" role="doc-biblioref">Music Encoding Initiative, Kepper, and Roland 2023b</a>)</span>． また，MEIは自筆譜や特定の出版譜などの一次資料に基づいたデジタル翻刻版の構築はもちろんのこと，複数の一次資料を参照して構築された学術校訂版のような二次資料の構築にも使用できることが宣言されている<span class="citation" data-cites="MusicEncodingInitiative2023b">(<a href="references.html#ref-MusicEncodingInitiative2023b" role="doc-biblioref">Music Encoding Initiative, Kepper, and Roland 2023c</a>)</span>．楽譜資料というと五線譜そのものに注目が集まりがちだが，書籍として出版される楽譜は五線譜以外の部分にもたくさんの情報を含んでいる．楽譜そのものの書誌情報をはじめ，目次や楽曲の紹介文，校訂報告など，書籍としての楽譜が持つ情報に加え，実際に演奏に使われた場合には作曲家や演奏家のメモ書きなどが残っていることもある．このような，情報はその楽譜がどのような資料なのかということを知る上で非常に重要であり，作品研究，音楽家研究の双方にとって貴重な資料になる．特に作曲家による最終判断を反映した楽譜の姿を目指して作成される原典版の編纂においては，あらゆる資料が学術的に調査され，音楽学者らによって非常に緻密な史料批判が展開される．音楽学における最先端の調査結果が詰め込まれた研究成果であり，楽譜以外にも多様な情報を含む原典版の内容そのものを機械可読に翻刻するためには，楽譜だけに注力したデータフォーマットではなく，より広く音楽情報全体を包括的に扱うデータフォーマットが求められる．</p>
<p>さらに，MEIは音楽学コミュニティからの要求に応えるため，CMN以外の記譜法にも対応しており，現行のMEI5.0は定量記譜法（mensural notation），ネウマ譜，タブラチュアをサポートしている．また，CMOプロジェクトではオスマン古典音楽やアルメニア教会音楽で用いられるHamparsum記譜法を取り入れることを目指しており<span class="citation" data-cites="Plaksin2019">(<a href="references.html#ref-Plaksin2019" role="doc-biblioref">Plaksin and Olley 2019</a>)</span>，今後もサポート対象となる記譜法は増えていくだろう．</p>
</section>
<section id="楽譜コンテンツの機械可読化を軸とするデジタル楽譜の類型" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="楽譜コンテンツの機械可読化を軸とするデジタル楽譜の類型">楽譜コンテンツの機械可読化を軸とするデジタル楽譜の類型</h4>
<p>デジタル媒体で扱われる楽譜と音楽情報の関係性について，Kepper<span class="citation" data-cites="Kepper2011">(<a href="references.html#ref-Kepper2011" role="doc-biblioref">2011</a>)</span>は電子版(elektronischen Ausgaben)とデジタル版（digitalen Ausgaben）という2つの類型を示している．電子版は既存のコンテンツを新しい媒体に適応させるだけなのに対して，後者はデジタルメディアを利用することにより自覚的であり，デジタル媒体だからこそ可能な情報の表現を目的に作成されたものであるとし，その重要な特徴の1つとして楽譜コンテンツそのものの機械可読化を挙げている．</p>
<p>OPERAプロジェクト <a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a>， RWA <a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a>， WeGA <a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a> などと協力関係にあるデジタル学術版制作のためのツール開発プロジェクトのEdiromやNMA Online <a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a> のような原典版として編纂された楽譜の公開が電子版に分類されていることから，電子版においては書籍という紙メディアとの親和性を維持しつつ，デジタル化によって印刷版を補完もしくは強化し，紙という物理的なメディアによって生じる制約の一部から音楽情報を解放することが重視されていることが分かる．</p>
<p>Kepper<span class="citation" data-cites="Kepper2011">(<a href="references.html#ref-Kepper2011" role="doc-biblioref">2011</a>)</span>が挙げている電子版の例では，いずれもプロジェクトのメインとなる原典版は紙による出版が行われており，デジタルメディアはそれに付随する校訂報告や資料を効率よく提示する役割にとどまっている．もちろん，校訂資料編がデジタルメディアで提供されることで，楽譜と校訂資料を行き来する際に大量のページをめくる必要はなくなり，紙面や印刷コストの制約を受けずに校訂編集者が望む校訂情報を余すことなく提供可能になるなど，楽譜と校訂情報が分離したことによるメリットも存在する．また，利用者側にとっては，大量の校訂情報を従来の索引よりも早く柔軟に検索することが可能になった．さらに，楽譜本体を出版することで，デジタルのメリットをユーザーに提供しつつ，新しい校訂版の発行によってもたらされてきた楽譜出版社の利益も同時に守ることができるというメリットもある．作品としてはパブリックドメインが大半を占めるクラシック音楽楽譜出版にとって，学術校訂版は重要な“新譜”であり，また国によっては学術編集版，本文批評版，原典版と呼ばれる出版物にオリジナルの著作者に対する権利とは別に，著作権法上の保護期間を設けている場合もあり <a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a> ，ロイヤリティの面からも出版を目指すインセンティブが存在している．</p>
<p>一方，デジタル版はデジタルメディアの特徴を生かし，コンテンツの質的な面において書籍メディアで展開されてきた既存のコンテンツとは異なるアプローチが模索され，書籍という印刷メディアを用いた出版では困難な，デジタルメディアが持つマルチモーダルな機能を活かしたコンテンツの拡張性が追求される．楽譜データも含め，コンテンツが紙面上に固定されることはなく，ソースとして提供されるデータをユーザーが探索し，さらなる学術研究の出発点として機能することが期待されている．現在公開されているプロジェクトの中に，Kepper<span class="citation" data-cites="Kepper2011">(<a href="references.html#ref-Kepper2011" role="doc-biblioref">2011</a>)</span>が指摘するデジタル版に相当するものは多くないが，国際モーツァルテウム財団が整備を進めるDIME <a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a> やベートーヴェンの遺伝的テキスト批評に取り組むBeethovens Werkstattはデジタル版楽譜と呼べる数少ないプロジェクトの一部である．電子版として挙げられてきたプロジェクトが最終的な成果物として印刷された楽譜の出版を目指し，デジタル技術はそれを補う存在として位置づけられているのに対して，デジタル版は紙での出版を前提とせず，デジタル技術を用いて新たな学術成果を生み出すことを目指している．</p>
<p>電子版もデジタル版も五線譜というフォーマットで表現されるコンテンツを扱っていることは変わらないが，楽譜そのものが機械可読に翻刻されることで楽譜の持つ拡張性や利用場面は大きく広がりうる．例えば，印刷物としての楽譜を整備した上で，校訂作業に関する説明や校訂資料をデジタル媒体で提供する場合，デジタルメディアで提供される後者の情報と印刷物として発行された校訂楽譜そのものをつなげるプロセスには人間の介入が必ず求められる．これは，作品を単位とする情報の提示であっても，作品内部に存在するモチーフや特定の記号に対する情報を提供する場合でも同様である．</p>
<p>一方，楽譜そのものを機械可読に翻刻することによって，楽譜とその外部に存在する情報の接続を人間に依存する必要がなくなり，人間は情報の探索に集中することができる．作品を単位に構成されるメタデータや出典情報は当然のこと，楽譜の内部に存在する特定のフレーズや記号を一意に指定し，そこに対して外部の情報を紐づけることが可能になる．これは，従来分断されてきた楽譜の記述内容そのものと楽譜外のコンテンツの接続が明示化されるというだけにとどまらず，楽譜外部に存在する情報に対して楽譜上の時間軸に基づいたタイムスタンプが付与され，校訂情報にも時間軸が生じるということになる．もちろん，全ての校訂情報が楽曲の持つ時間軸に紐づけられるわけではないが，校訂資料に対して時間の概念が導入され，それが機械可読に探索可能になることで，楽譜を軸にした情報の探索がより柔軟になることが期待できる．また，校訂報告のテキストだけではなく，楽譜そのものが検索対象になることで，利用者は特定のフレーズ，和声進行，リズムパターンなど，より音楽的な側面からの情報探索が可能になる．</p>
<p>Müller<span class="citation" data-cites="Müller2021">(<a href="references.html#ref-Müller2021" role="doc-biblioref">2021</a>)</span>が指摘するように，音楽情報の中には印刷楽譜，シンボリックデータ，オーディオデータなど，複数の表現が存在しており，それらは相補的な関係性を構築しながら音楽情報の全体像を構成している．MIDIもMusicXMLも音楽に関するデータであることは確かだが，実際の音楽情報はそこだけで完結しているわけではない．しかし，これらのデータフォーマットはその内部で複数の音楽表現に関連する記述を共存させるような仕組みを持っておらず，基本的には1つの表現に閉じた状態で完結している．一方，学術編集版の内部で表現されているような，楽譜とそれに関わる情報の接続を実現するためには，単なる“楽譜”の機械可読化よりも広く“音楽”に関する情報の記述を実現する必要がある．</p>
<p>楽譜記述に基づいた表現の拡張は，先に指摘したPAEが抱える正規化に関する問題ともつながる．異読，異版によって生じる楽譜出版物の差分を表現することは資料のfindabilityを向上させる上で重要であることもさることながら，書誌学や文献学にとってはそれ自体が本質的な意味を持つ重要な作業である．音楽学における原典版の編纂事業も時代，地域，校訂者などの違いによって少しずつ異なる同一作品の出版楽譜や，作曲家自身が残した複数バージョンのスケッチ，草稿，浄譜などを比較し，その違いを明らかにしながら作曲者の意図を反映した楽譜を目指していくが，その過程で行われているのは，ある1つの楽曲に対して複数存在する楽譜を比較し，その差分について検討する作業である．</p>
<p>Kepper<span class="citation" data-cites="Kepper2011">(<a href="references.html#ref-Kepper2011" role="doc-biblioref">2011</a>)</span>が指摘するデジタル版を実現する上で重要なのは，このように機械可読に翻刻された楽譜と楽譜の外側に存在する音楽に関連した様々な情報とを並列に扱いながら，それらが相互に接続されたメディア環境を実現することである．</p>
</section>
</section>
</section>
<section id="音楽情報の機械可読化に向けたアプローチ" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="音楽情報の機械可読化に向けたアプローチ"><span class="header-section-number">5.2</span> 音楽情報の機械可読化に向けたアプローチ</h2>
<p>紙メディアをベースに培われてきた習慣をデジタルメディア上で再構成することは，伝統的な人文学コミュニティへデジタル技術を導入するために避けて通ることのできないステップであると同時に，人文系の学問領域からデジタル技術に対してもたらされる要求に応えるためにも必要不可欠である． 学術編集版がデジタル技術によって拡張されていく状況は不可逆的なものであろう．音楽学コミュニティからもたらされるこのような要求をMEIが満たしうるのは，MEIがコミュニティ全体として人文学研究に求められる情報の質的な信頼性を確保するために，音楽学をはじめとする既存の人文研究の領域との連携を重視し，MusicXMLやMIDIとは異なるデータ記述のモデルを採用しているためである．MIDIやMusicXMLが音楽資料の内部に含まれる情報の内，特定の領域に焦点を当てて情報を記述しているのに対して，MEIは楽譜とそれに関連付けられた様々な情報や知識を1つのまとまりとして機械可読な形式で記述することを目指している．</p>
<p>このような，音楽情報全体を統合的に扱うデータモデル自体はMEIのコミュニティによって生み出されたわけではなく，音楽とコンピュータの関係性が始まった直後には既に存在した主題であり，それはMIDIが登場した1981年を大きく超えて遡ることができる．MIDI，MusicXML，MEIのいずれも，そのような先行する事例を少しずつ踏襲しながら，音楽記述に対する自らの態度を確立してきた．MIDIがシンセサイザーの同期という楽器制御に関する文脈から，MusicXMLが楽譜ソフトウェア間のファイル共有という楽譜組版の文脈から生じてきたことはすでに触れてきた．ここからはMEIのような学術的な楽譜記述モデルがどのような文脈から生じ，そのモデルがどのように構築されてきたのかについて考察していく．</p>
<section id="音楽情報を構成する領域" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="音楽情報を構成する領域"><span class="header-section-number">5.2.1</span> 音楽情報を構成する領域</h3>
<p>音楽が本質的に人間の聴覚に対して訴える表現であり，楽譜に並んだ記号を“音楽作品”と同一視して扱うことは出来ないと指摘したが，それは音楽作品の概念に対する議論であり，音楽に関わる表現や情報そのものは，楽譜に代表される視覚的もしくは記号的な情報，作品演奏やその鑑賞によって生じる時間的な情報など，聴覚的な要素に限らない複合的な表現から成り立っている． 特にコンピュータで音楽情報を扱う場合，音楽を構成する様々な要素を機械的に処理可能な形式で表現する必要があり，そのモデル化はコンピュータが扱いうる音楽のある側面を明確に定義するうえでも重要である．</p>
<p>コンピュータを用いた音楽情報処理の文脈における音楽情報のモデル化に関する最初期の議論として，Babbitt<span class="citation" data-cites="Babbitt1965a">(<a href="references.html#ref-Babbitt1965a" role="doc-biblioref">1965</a>)</span>による音楽情報区分が挙げられる．Babbitt<span class="citation" data-cites="Babbitt1965a">(<a href="references.html#ref-Babbitt1965a" role="doc-biblioref">1965</a>)</span>はコンピュータで音楽を扱うにあたって，音楽の記譜法や楽譜に関連する側面である書記的（graphemic）領域，周波数，振幅，持続時間など，音としての物理的な側面である音響的（acoustic）領域，人間が音をどのように知覚し，理解するかという聴覚的（auditory）領域を区別することを提起している．Babbitt<span class="citation" data-cites="Babbitt1965a">(<a href="references.html#ref-Babbitt1965a" role="doc-biblioref">1965</a>)</span>の議論はコンピュータを用いたシェンカー分析に取り組んでいたMichael Kassler <a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a> に影響を受けており，各ドメインを超えて音楽情報を解釈することはできないと主張している．コンピュータで音楽を扱うという文脈ではあるが，主張そのものは，楽譜という記号を通して聴覚的刺激である音楽を解釈することは出来ないという議論とほぼ重なるものである．</p>
<p>楽譜も含む包括的な音楽情報を複数の領域に分割して取り扱う考え方は，作曲支援音楽編集ソフトウェアであるMockingbird <a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a> の開発に取り組んだMaxwell<span class="citation" data-cites="Maxwell1981">(<a href="references.html#ref-Maxwell1981" role="doc-biblioref">1981</a>)</span>が取り込み，音楽情報を 物理領域(physical domain)，論理領域 (logical domain)，図像領域 (graphical domain)の3つに類型化するモデルが提示された．さらに，Steven Newcomb，Charles Goldfarbらが提案したSMDLではMaxwell<span class="citation" data-cites="Maxwell1981">(<a href="references.html#ref-Maxwell1981" role="doc-biblioref">1981</a>)</span>の三領域モデルを拡張し，音楽情報を論理（logical）領域, ジェスチャ（gestural）領域, 視覚（visual）領域, 分析（analytical）領域の4つに類型化するモデルが採用され，SGML <a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a> から派生したHyTime <a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a> に準拠した包括的音楽記述規格が目指された．SMDLにはISO/IEC DIS 10743というドラフト番号が割り当てられているが最終的な標準化には至っていない．しかし，ミラノ大学コンピュータサイエンス学部の音楽情報学研究室のサイトで草稿が公開されており，その他複数の関連資料も確認できる<a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a>．</p>
<p>ISO規格にはならなかったものの，SMDLが提示する音楽情報の四領域記述モデルはMEIにも参照されており，現在の音楽情報記述における基本的な枠組みとして受け継がれている．</p>
</section>
<section id="smdlにおける音楽情報の四領域記述モデル" class="level3" data-number="5.2.2">
<h3 data-number="5.2.2" class="anchored" data-anchor-id="smdlにおける音楽情報の四領域記述モデル"><span class="header-section-number">5.2.2</span> SMDLにおける音楽情報の四領域記述モデル</h3>
<section id="論理領域" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="論理領域">論理領域</h4>
<p>SMDLにおける論理領域の概念は，楽譜と演奏の両方に共通する抽象概念とは何か，という問いに対する答えと見なすことができるとされる．それは，「音程，リズム，ハーモニー，ダイナミクス，テンポ，アーティキュレーション，アクセントなどに関する作曲家の意図」であり，「ジェスチャ領域と視覚領域の両方に共通する抽象的な情報」と説明される． より具体的に言うと，楽譜が持つ視覚的な記号によって定式化される拍に基づいた時間軸とは異なる仮想時間軸が用意され，その上に1つの作品を構成する基本的かつ本質的な音楽的構造である論理領域の要素が配置されていく．楽譜上で可視化される拍節に基づく時間軸や特定の演奏が持つ時間軸を基準としないのは，第一にそれらが1つの作品に対して複数存在する状況が否定できないためである．また，記譜法や演奏慣行は時代によって変化するものであり，その例としてSMDLマニュアルでは“定量記譜における音価のインフレーション”(metric notational inflation)を挙げている．実際，時代を遡ると現在では使用されていない全音符(semibreve)以上の音価を持つbrevis, longa, maximaといった音符記号に遭遇することがあり，時代が下るにつれて音符が持つ音価は細分化され，長い音価を持った音符が使用されなくなっていった．これに伴って，拍子記号の記述方法も変化している．楽曲を記述する際の時間軸を楽譜内部に依存すると，このような変化に対応するためにフォーマットの一貫性を犠牲にせざるを得ない状況が生じうる．</p>
<div id="fig-no27" class="lightbox quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-no27-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/中世音符.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;5.4: brevis, longa, maximaを含む音楽入門書の例 [@Morley1597, p.9]"><img src="img/中世音符.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-no27-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.4: brevis, longa, maximaを含む音楽入門書の例 <span class="citation" data-cites="Morley1597">(<a href="references.html#ref-Morley1597" role="doc-biblioref">Morley 1597, 9</a>)</span>
</figcaption>
</figure>
</div>
<p>SMDLの論理領域は任意の数のcantus <a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a> によって構成され，それぞれのcantusは音符，休符，和音などのスレッド要素，歌詞や音節情報を含む歌詞要素，テンポ指示を含むバトン（baton）要素 <a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a> ，音楽構造として定義されるアーティキュレーション，ミュートなどの指定やモジュレーション，フィルタの適用など，音響出力に対する加工や修正を記述するワンド要素によって構成されており，それぞれの始点と終点が仮想時間軸上で指定される．これによって，楽譜によって表現される視覚的要素と音楽の構造が分離される<span class="citation" data-cites="Sloan1997">(<a href="references.html#ref-Sloan1997" role="doc-biblioref">Sloan 1997, 488–89</a>)</span>．ここでは，説明のために，音符，休符，和音など，楽譜上の可視化された記号に対する名称を挙げたが，論理領域で扱うのはその記号の持つ意味的な側面だけであって，その記号がどのような形状で表現されるかということは問われない．つまり，演奏や楽譜による具体的な音楽の表象や楽曲に対する分析などは，すべて論理領域の記述を起点とする派生物であると理解できる．</p>
<div id="fig-no28" class="liightbox quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-no28-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/cantus.png" class="liightbox img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-no28-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.5: SMDLにおける論理領域の構造<span class="citation" data-cites="Sloan1997">(<a href="references.html#ref-Sloan1997" role="doc-biblioref">Sloan 1997, 488</a>)</span>
</figcaption>
</figure>
</div>
<p>SMDLが論理領域を構成するcantusに複数性を認めているということは興味深い妥協であり，音楽を取り巻く文化の複雑性を反映していると言える．cantusが2つ以上含まれる例としてマニュアルでは以下のように示されている．</p>
<blockquote class="blockquote">
<p>Normally there is one cantus in the content of each workseg. If there is more than one, it is usually because there are different sources for the essential music information of the piece, and the sources disagree so much that it is impractical to represent them both in the same cantus element, while accounting for the differences by means of ossias . Needless to say, as in the case of ossias, a performer must choose which cantus to render in any given instance.</p>
</blockquote>
<blockquote class="blockquote">
<p>通常，各worksegには1つのcantusが含まれる．2つ以上のcantusが含まれるのは，その楽曲の本質的な音楽情報の典拠が複数あり，それらの記述が大きく食い違っているために，ossiaを用いてそれらの差分を考慮しつつ，同一のcantus要素でその両方を表現する事が現実的ではないためである．言うまでもないが，ossiaの場合と同様に，演奏者は任意のケースにおいて，どのcantusを演奏するかを決定する必要がある．</p>
<div style="text-align:right">
<p>（著者訳）</p>
</div>
</blockquote>
<p>つまり，SMDLの論理領域は仮想的な時間軸の導入や楽譜の持つ視覚的要素との分離など，抽象性の高い記述を可能とする一方で，その内容を何らかの典拠に拠る可能性を認め，さらに作曲家の意図であるとみなしうる典拠情報に複数性が認められる場合には，それらのすべてを記述することを可能としている．すなわち，原典版編纂事業が展開してきた作曲家の意図をめぐる議論を新たにSMDLというメディア環境で実践し，その記述をより精緻かつ抽象的な次元に引き上げることを可能にする一方で，デジタル翻刻を行う主体が，そこから楽譜と演奏の両方に共通する抽象的な概念性を見出し，音楽作品に対する作曲家の意図が反映された典拠資料として採用したものは，論理領域の構成要素として取り込まれうるということである．</p>
<p>データ記述において軸となる楽譜の記述として複数の参照先を取り込むことを認めているSMDLの論理領域は，それ自体が楽曲という単位で複数の典拠資料を共存させる情報結節点としての役割を担っているといえる．そこでは，ある特定の楽譜資料が中心として権威づけられることも，デジタル化された楽譜データそのものが新しい中心として権威づけられることもなく，複数の楽譜資料がそれぞれの立場から情報を提供し，それらの集合体が論理領域を構成することで，楽曲の持つ複数の側面が同時に記述されることが可能となるのである．</p>
<p>ただし，1つのSGMLファイルの内部に記述しうる楽曲そのものに対する明確な定義が与えているわけではなく，論理領域に複数のcantusを含みうるという点については，あくまでも1つの手段として提供されているに過ぎない．したがって，同一楽曲として扱いうる楽曲を分析や情報整理の観点から複数のファイルとして扱うことも許容している．この点については，利用者側の判断に委ねられており，各ユーザーが自らの目的に照らして適切な翻刻の方針を選択することが求められる．</p>
</section>
<section id="ジェスチャ領域" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="ジェスチャ領域">ジェスチャ領域</h4>
<p>ジェスチャ領域には「演奏者によって追加される情報」もしくは「特定の演奏によってもたらされる音響としての音楽」が記述される． ジェスチャ領域は，任意の数のパフォーマンスが含まれており，あるパフォーマンスにおいて論理領域の構成要素が，演奏者によって，いつ，どのように音響としてレンダリングされるかが記述される． すなわち，音程のゆらぎ，テンポの緩急，アクセントの付け方など，演奏者が実際に音楽を表現するために用いる手段のすべてがここに含まれる． また論理領域によって定義される仮想時間軸と演奏によって生じる実時間軸の関係をマッピングすれば，論理領域の構成要素がどのように演奏によって具現化されているのかを記述することができる．</p>
<p>これらの情報が，“音響領域”や“演奏領域”ではなく，“ジェスチャ”すなわち“身振り”として記述されていることからは，SMDLが音楽を記述する際に人間の身体性をより中立な存在として介在させようとする姿勢が読み取れる．先に挙げた，Maxwell<span class="citation" data-cites="Maxwell1981">(<a href="references.html#ref-Maxwell1981" role="doc-biblioref">1981</a>)</span>はphysical, logical, graphicalという3つの類型を言語処理のプロセスになぞらえ，発話をテープレコーダで記録するphysical領域，構文解析などのテクニックによってそれを理解するlogical領域，それらを視覚的な表現である文字列として生成していくgraphical領域として説明している．これは，Maxwell<span class="citation" data-cites="Maxwell1981">(<a href="references.html#ref-Maxwell1981" role="doc-biblioref">1981</a>)</span>が開発したMockingbirdが作曲の支援を目的としており，作曲家が思い浮かべた音像をコンピュータでキャプチャするところから全体のワークフローがデザインされていたためであると推察できるが，より一般化された音楽記述を目指す場合は作曲家の着想ではなく，作品そのものの存在を単位としたうえで，そこに演奏とその結果としての音響を音楽情報の構成要素として織り込んでいくSMDLのようなアプローチを採用したほうがより汎用的で中立な記述が実現されるだろう．</p>
</section>
<section id="視覚領域" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="視覚領域">視覚領域</h4>
<p>ジェスチャ領域が演奏者によって追加される情報を記述するのに対し，視覚領域には「編集者，版彫職人，組版職人によって加えられた情報」，あるいは「ある特定の版において音楽が実際にどのように可視化されているか」が記述されていく．ジェスチャ領域が複数の演奏を含みうるように，視覚領域にも複数の楽譜を含むことができ，各種記号の配置，フォントの指定，マージンや譜表の改行，改ページ位置などのページレイアウト，連桁，棒の向きなどの規則や例外など，視覚的な詳細事項が含まれる．これらの情報は，論理領域の記述をどのようにレンダリングするかという規範的な視点と，既存の楽譜が論理領域の記述をどのようにレンダリングしているかという記述的な視点の2つの側面から記述されうる．</p>
</section>
<section id="分析領域" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="分析領域">分析領域</h4>
<p>分析領域は他の3つの領域の情報の一部もしくはすべてに関連した見解や解釈を含む，任意の数の理論的な分析や注釈で構成されている．他の領域同様，分析領域も複数の要素を内包することができる． Newcomb<span class="citation" data-cites="Newcomb1991a">(<a href="references.html#ref-Newcomb1991a" role="doc-biblioref">1991</a>)</span>は分析領域を「音楽理論家の遊び場」と例えており，音楽の構造，楽譜上の記号，特定の演奏に対する理解や考察によってもたらされた“知識”が記述される．この領域は作曲支援ツールを開発したMaxwell<span class="citation" data-cites="Maxwell1981">(<a href="references.html#ref-Maxwell1981" role="doc-biblioref">1981</a>)</span>が採用した三領域モデルには含まれておらず，SMDLが音楽に関連したより広い情報の記述を目指していることがわかる．</p>
<div id="fig-no28" class="lightbox quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-no28-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="img/SMDL_Structure.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;5.6: SMDLにおける論理領域とその他3領域の関係。矢印の向かう方向を参照していることを示す．[@Sloan1997, p.471]"><img src="img/SMDL_Structure.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-no28-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.6: SMDLにおける論理領域とその他3領域の関係。矢印の向かう方向を参照していることを示す．<span class="citation" data-cites="Sloan1997">(<a href="references.html#ref-Sloan1997" role="doc-biblioref">Sloan 1997, 471</a>)</span>
</figcaption>
</figure>
</div>
</section>
</section>
<section id="論理領域と視覚領域の分離が求められた背景" class="level3" data-number="5.2.3">
<h3 data-number="5.2.3" class="anchored" data-anchor-id="論理領域と視覚領域の分離が求められた背景"><span class="header-section-number">5.2.3</span> 論理領域と視覚領域の分離が求められた背景</h3>
<p>SMDLでは，論理領域と視覚領域がより明確に分離されており，楽曲の抽象的な構造を記述する論理領域を“楽譜としてどのように可視化するか”という側面から視覚領域が定義されており，この記述からは論理領域の記述が楽譜をレンダリングする際に参照されることが1つの前提として想定されている．これは，楽譜によって表現される音楽の論理的な構造とそれを可視化するための方法を分離して記述するという四領域モデルのデザインに起因するというよりは，西洋音楽が用いる五線譜をコンピュータで扱う際に必然的に直面する記号置換の必要性に対処するために生み出された一種の習慣といえる．</p>
<p>雅楽譜をはじめ，邦楽譜の多くは楽譜記述のために用意された専用の記号体系を持たず，自然言語でも用いられる文字を用いて記述されていく．テキスト資料は，特殊文字などを除けば，そこに書かれた情報は概ね直接プレーンテキストとして文字起こしができる <a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a>． 一方，当然ではあるが，五線譜は専用にデザインされた独自の記号や記述体系を有しており，それらをプレーンテキストで直接文字起こしすることはできない．したがって，何らかの形で五線譜の記述を文テキスト化しようとする場合には，音楽記号が配置された楽譜の状況から人間が読み取った音楽の“構造”を記述することが常に求められてきた．</p>
<p>先に取り上げたPAEは音楽目録の作成という限られた用途を想定して五線譜上の記述から読み取った内容をプレーンテキストとして記述する枠組みを提供していたが，より一般的な用途を目的としたプレーンテキストによる音楽構造の記述形式としてはABC記譜法が挙げられる．ABC記譜法は民謡採集を当初の目的として1993年にリリースされたフォーマットであり，冒頭に楽曲のID（<code>X</code>），タイトル（<code>T</code>），拍子記号（<code>M</code>），1文字が小節内において占める音価（<code>L</code>），調（<code>K</code>）が簡易的なメタデータとして示され，その後に西洋音楽が用いるCDEFGABの音名を用いた楽譜が続く<span class="citation" data-cites="Walshaw2024">(<a href="references.html#ref-Walshaw2024" role="doc-biblioref">Walshaw 2024</a>)</span>．</p>
<p>プレーンテキストを用いた旋律の記述が可能であることからインターネット上で広く利用されており，公式サイト <a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a> には民謡や古楽を中心に約730,000曲の楽譜が公開されている<span class="citation" data-cites="CenterforComputerAssistedResearchintheHumanities2010">(<a href="references.html#ref-CenterforComputerAssistedResearchintheHumanities2010" role="doc-biblioref">Center for Computer Assisted Research in the Humanities 2010</a>)</span>．</p>
<div id="fig-no29" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-no29-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode default code-with-copy"><code class="sourceCode default"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>X:1</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>T:Ah! Vous dirais-je, Maman</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>M:4/4</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>L:1/4</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>K:C</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>C C  G G | A A G2 | F F E E | D D C2 |</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>G G  F F | E E D2 | G G  F F | E E D2 |</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>C C  G G | A A G2 | F F E E | D D C2 |]</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-no29-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5.7: ABC記譜法による『きらきら星』の譜例
</figcaption>
</figure>
</div>
<p>このように，楽曲の構造をテキスト化することで音楽を記述した場合，元の楽譜が持っていたレイアウトや記号に関する情報は失われてしまう．したがって，これらを再び五線譜に翻訳するためには楽曲の構造的な記述をどのように元の記号に置き換えるかという定義が求められる．標準入出力として文字情報の符号化に関する標準化がいち早く進んだ一方で，楽譜の場合はプレーンテキストに相当する標準的な記述が存在していない．そのため，楽譜や音楽に関わる情報は常に既存の仕組みを応用しながら，楽譜用のプレーンテキストに相当するデータ記述を現在に至るまで追求し続けている．このように，五線譜によって表現される情報の内容をコンピュータで扱う場合には，音楽の構造に関する記述とその視覚的構成は切り離されざるを得ないのである．</p>
</section>
<section id="音楽情報の記述に際して生じる資料の中心性に関する問題" class="level3" data-number="5.2.4">
<h3 data-number="5.2.4" class="anchored" data-anchor-id="音楽情報の記述に際して生じる資料の中心性に関する問題"><span class="header-section-number">5.2.4</span> 音楽情報の記述に際して生じる資料の中心性に関する問題</h3>
<p>SMDLにおける論理領域には複数のcantus要素を含むことができるように設計されており，それぞれのcantus要素は異なる楽譜資料を参照することができる．このような，情報記述の基底をなす資料を複数定義することができる仕組みは，音楽情報の記述において生じる資料の中心性に関する問題に対処する上でも有用な構造といえる．情報記述の基準となる要素に複数性が認められるということは，論理領域を構成している楽曲構造の記述が唯一の基準ではないという可能性を常に示唆しており，論理領域に記述された楽曲構造が持つ情報結節点としての中心性は，あくまでもその情報記述の内部においてのみ生じるものであるということを示している．</p>
<p>このような情報や知識の接続における軸や中心性に関する問題は，正しい本文を追求する校訂作業の文脈で特に慎重に議論されてきており，デジタル化された複数の資料を相互に関連付けた結果生じてしまう資料の中心性をどのように解釈するかという点について様々な立場がありえる．</p>
<section id="学術編集における底本の存在の扱い" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="学術編集における底本の存在の扱い">学術編集における底本の存在の扱い</h4>
<p>学術編集における底本の存在について議論を展開したGreg<span class="citation" data-cites="Greg1950">(<a href="references.html#ref-Greg1950" role="doc-biblioref">1950=2016</a>)</span>は，著者が書いたものに最も近いと考えられる現存の本文を選択し，それに最小限の変更を加えるのが編纂作業であるとし，そこでは「本文の読みに際して極めて重要であり，書かれている事柄に関する著者の意図や表現の本質に関わる要素」である“substantive readings”と，「綴り・句読法・分綴法など，主として形式面に関わり，非本質的もしくは附随的と見なされるもの」である“accidentals”が区別されると述べている．私たちが目にしている写本や出版物を生み出した写字生や植字工もsubstantive readingsとaccidentalsの区別を前提に作業を行ってきており，accidentalsについては原本の記述を無視して彼らの慣例や好みに従う傾向が強いのに対して，substantive readingsについては，不注意の写し間違えや何らかの理由で原本をそのままに写さないケースがあったとしても，彼らが原本のsubstantive readingsを正確に再現することを目指していたという点について疑いの余地はないとしている．Greg<span class="citation" data-cites="Greg1950">(<a href="references.html#ref-Greg1950" role="doc-biblioref">1950=2016</a>)</span>は“substantive readings”と“accidentals”の区別を強調したうえで，特定の本文を底本として選択して編纂を進めるのは文献学や言語学の研究における便宜的処置であり，底本は編集そのものの軸として何らかの中心性を有するものではなく，それにとらわれることなく自由にsubstantive readingsを選択していくことは編纂者の責務であるとしている． Greg<span class="citation" data-cites="Greg1950">(<a href="references.html#ref-Greg1950" role="doc-biblioref">1950=2016</a>)</span>の主張は，これまで音楽学が積み上げてきた原典版編纂の習慣にも通じるものである．原典版の編纂者は大量の資料を収集し，それらに対する批判的校訂を通して最も作曲者の意図を反映していると思われる本文を再構成することで，既存の資料のいずれかを中心に据えることなく参照点となりうる軸を構築している．こうして確立された原典版は校訂の土台となる底本を超えたある種の権威としての地位を確立し，その後に続く研究の基盤となると同時にそれを軸に既存の研究や議論が接続，再構成される結節点としての機能を果たしている．</p>
<p>一方，音楽情報の関係性を厳密に記述するのであれば，ある特定の楽譜を軸に展開された音楽実践の関係を記述するのでない限り <a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a> ，楽曲とそれに関連した情報の間に存在する関係性は，特定の楽譜を軸とする，楽譜–楽譜や楽譜–演奏といった関係ではなく，楽曲–楽譜，ないし，楽曲–演奏という関係性によって紐づけられるはずである．例えば，ある演奏がどの楽譜との関係性によって記述されうるかということは必ずしも定義されない一方で，それがどの楽曲の演奏であるかという点はほとんどの場合において明らかである．同様に，ある楽譜の存在は抽象的な楽曲概念に対する具体的な表象の一形態であり，必ずしも他の楽譜との関係性において位置づけられるものではない．したがって，原典版がどれだけ権威付けられた存在であろうとも，本来はその具体的な楽譜記述を楽曲概念と重ねて扱うことは出来ないはずである．</p>
<p>原典版のような権威付けられた存在である楽譜を楽曲概念と同一視し，その記述を中心に据えた論理領域の記述を前提にしてしまうと，原典版以外の資料は原典版からの差分によって記述される状態を抜け出すことができず，常に権威付けられた楽譜の子要素として位置づけられることになる．このような状況は，Greg<span class="citation" data-cites="Greg1950">(<a href="references.html#ref-Greg1950" role="doc-biblioref">1950=2016</a>)</span>が論じた底本からの自由とは対照的であり，デジタル楽譜によって既存の原典版を相対化し，それに対する新たな視点が提示される可能性を排除してしまう．また，このような態度は，紙面という物理的な制限から解放されたことで，複数の底本をより柔軟に取り込みながら楽曲の構造を記述することが出来るはずであるデジタル楽譜の潜在的な可能性をも排除してしまう．</p>
<p>実際，楽曲概念を軸とした関係性の記述は必ずしも常に厳密に運用可能なものではない．非常に抽象的な論理領域の記述を想定しているSMDLでさえ，何らかの典拠となる既存の楽譜を参照して論理領域を構成することを想定し，かつそこに複数性を認めているように，楽曲とその周囲に存在する情報との関係性を記述するうえで，信頼できる複数の楽譜の存在は非常に大きな役割を果たしうる．すなわち，SMDLにおける論理領域によって定義される楽曲の構造は，原典版の編纂のような複数の資料を接続しながらその情報を再構成することを可能にすると同時に，既存の権威付けられた楽譜自体を取り込みながら，そこに存在する中心性を相対化することで新たな視点を提示することも可能にしている．このように，SMDLの論理領域は音楽情報の結節点としての機能を果たすと同時に，その記述自体に何らかの権威性や中心性が生じることを複数性の許容という記述構造によって回避し，新たな情報や知識の接続をより中立的な立場で実現する基盤としての役割を担っている．</p>
</section>
</section>
<section id="meiにおける四領域モデルの適用" class="level3" data-number="5.2.5">
<h3 data-number="5.2.5" class="anchored" data-anchor-id="meiにおける四領域モデルの適用"><span class="header-section-number">5.2.5</span> MEIにおける四領域モデルの適用</h3>
<p>SMDLの四領域モデルのコンセプトはMEIにも取り入れられており，logical, gestural, visual, analyticalという用語についても共通して用いられている． しかし，<a href="#fig-no28" class="quarto-xref">Figure&nbsp;<span>5.6</span></a> を見ても分かる通り，SMDLが構築した四領域モデルは内部に複雑な参照関係を持っており，利用者にとって常に扱いやすい構造とは言い難い． また，Berndt<span class="citation" data-cites="Berndt2018">(<a href="references.html#ref-Berndt2018" role="doc-biblioref">2018</a>)</span>はSMDLが提起する4つの領域に属する情報の全てを記述しようとすれば，非常に複雑なデータ構造になることは避けられず，音楽研究を目的としたデジタル楽譜のニーズに応じる場合であっても，必ずしも4つの領域全てを満たす必要はないということを指摘している． MusicEncodingInitiative2023d<span class="citation" data-cites="MusicEncodingInitiative2023d">(<a href="references.html#ref-MusicEncodingInitiative2023d" role="doc-biblioref">2023d</a>)</span>自身も，MEIスキーマ全体で四領域モデルの概念をすべて利用できるわけではなく，2つの領域が重複しているケースが非常に多いとしている．また，部分的に論理領域と視覚領域が統合されているために，視覚領域がジェスチャ領域よりも優先される場合があるなど，MEIはSMDLが定義する概念体系を引き継ぎながらも，音楽学研究での利用を想定して構造の一部を簡略化している． それでもなお，Berndt<span class="citation" data-cites="Berndt2018">(<a href="references.html#ref-Berndt2018" role="doc-biblioref">2018</a>)</span>はMEIが提供する以下のような特徴が故に処理系の実装が複雑にならざるを得ず，それがデジタル学術校訂以外のコミュニティでのMEIの利用を妨げる要因になっていると指摘している．</p>
<blockquote class="blockquote">
<ul>
<li>同一の情報が異なる方法で記述されうる．</li>
<li>論理領域とジェスチャ領域が相補的にも競合的にも記述されうる．</li>
<li>記述の標準化が徹底されておらず，曖昧さを一部で許容している．</li>
</ul>
</blockquote>
<p>特に，1点目と3点目に挙げられている問題はコンピュータによる処理が煩雑になるばかりでなく，データを作る人間側にとってもコストが高く，データ内における記述の一貫性を保つことを難しくしている．Berndt<span class="citation" data-cites="Berndt2018">(<a href="references.html#ref-Berndt2018" role="doc-biblioref">2018</a>)</span>は，その具体例として，音高，長さ，調，移調に関する情報が必ずしも音符または休符要素に直接配置されるとは限らず，小節の過不足，入れ子になった三連符や反復記号，楽譜の記述順序とは異なる演奏順序を指示するダ・カーポやダル・セーニョなど，多様で時に例外的な楽譜記述に対応することを目的に用意されている仕組みが，かえって構造記述の曖昧さを助長していると指摘している．</p>
<section id="meiガイドラインであることの功罪" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="meiガイドラインであることの功罪">MEI“ガイドライン”であることの功罪</h4>
<p>MEIに含まれるこのような曖昧さは，MEIが標準規格ではなく，あくまでも楽譜記述のための枠組みを提供するガイドラインとして策定されていることに1つの要因を見出すことができる．MEIの目的は音楽情報のデジタル化に関わる問題に対して何らかの解決策を提供することであり，利用者に対して特定の記述方法を強制することではない．この点は，楽譜交換フォーマットであるMusicXMLや電子楽器間の通信プロトコルとして制定されたMIDIとは大きく異なる．</p>
<p>MEIは音楽情報をマークアップするためのツールや手段を提供するが，それをどのように運用するかは利用者側がそれぞれの目的に応じて決定しなければならない．ガイドラインにおいても以下のように，MEIのカスタマイズが推奨されている．</p>
<blockquote class="blockquote">
<p>In production, it is best to use a customized version of MEI, restricted to the very needs of a project. Such a custom schema will guide the encoders and will help to ensure consistency and data quality throughout a project’s files. [……] The customization will help to reflect the scope of a project into its data: Only those aspects of music notation a project is interested in will be allowed, so that the absence of a specific information can not be misunderstood as an oversight of the encoders. Larger editorial projects like Complete Works editions typically use Editorial Guidelines (german: Editionsrichtlinien) for the same purposes: (internal) quality control and (external) documentation. In that sense, MEI customizations may serve as Editorial Guidelines in digital form.</p>
<div style="text-align: right;">
<p><span class="citation" data-cites="MusicEncodingInitiative2023e">(<a href="references.html#ref-MusicEncodingInitiative2023e" role="doc-biblioref">Music Encoding Initiative, Kepper, and Roland 2023a</a>)</span></p>
</div>
</blockquote>
<blockquote class="blockquote">
<p>MEIを実際に使用する際にはプロジェクトのニーズに最適化されたカスタマイズ版を作成するのが望ましい．プロジェクトの目的に応じてカスタマイズされたスキーマはデータ作成者を適切なマークアップへと導き，作成されるファイル全体の一貫性とデータ品質の確保に役立つ． [……] カスタマイズによってプロジェクトが持つ関心領域をデータ記述に反映させることができる．つまり，プロジェクトが注意を向ける楽譜の側面のみが利用可能となり，特定の情報の欠落がデータ作成者のミスと誤解されることを防ぐことができる．一般的に全集のような大規模プロジェクトでは，作業品質の管理と編集方針ドキュメントの作成というほぼ共通した目的のために，編集ガイドライン（Editionsrichtlinien）が作成される．カスタマイズされたMEIスキーマはデジタル形式の編集ガイドラインとして役立つ可能性がある.</p>
<div style="text-align: right;">
<p>著者訳</p>
</div>
</blockquote>
</section>
<section id="音楽情報記述における曖昧さの必要性" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="音楽情報記述における曖昧さの必要性">音楽情報記述における“曖昧さ”の必要性</h4>
<p>つまり，MEIが共有するのは，音楽情報を記述するためのスタンスや方法論であり，同じMEIを使用していても，その運用方法は大きく異なる可能性があるということである．MIDIやMusicXMLが標準化されたデータ構造を提供することでシェアを伸ばしていったのに対し，利用者側にカスタマイズを推奨するMEIは利用者側にMEIの運用方法を自ら考えることを求めており，一般的な標準規格のように決められた方法に従っていれば良いという受け身なデータ運用を許容しない．MEIが提供する記述の柔軟性はそれを能動的に運用するコストに見合ったニーズを持ったコミュニティやプロジェクトにとっては有益である一方で，そうでないユーザーにとっては負担が高く，結果として得られるデータの一貫性や信頼性も十分に担保されているとは言い難いものになってしまうだろう．</p>
<p>それでもBerndt<span class="citation" data-cites="Berndt2018">(<a href="references.html#ref-Berndt2018" role="doc-biblioref">2018</a>)</span>はMEIの提供する詳細な楽譜記述は人文系の音楽研究はもちろんのこと，MIRのコミュニティにとっても有益であり，MEIの複雑さを乗り越えるべき明白な理由があるとしている．人文系の要求を満たしうる情報記述を目指しているMEIには従来MIRが中心的に扱ってきたMIDIやMusicXMLには含まれない情報を記述することができる．すなわち，そこにはこれまでMIRのコミュニティにおいて研究の俎上に載せられることが少なかった音楽学の研究に基づいた情報が含まれており，MEIを使用することで，MIRのコミュニティにこれらの情報を利用した新たな研究の可能性が広がることが期待されているのである．</p>
</section>
</section>
<section id="音楽情報の重要性とmirの課題" class="level3" data-number="5.2.6">
<h3 data-number="5.2.6" class="anchored" data-anchor-id="音楽情報の重要性とmirの課題"><span class="header-section-number">5.2.6</span> 音楽情報の重要性とMIRの課題</h3>
<p>単にコンピュータで処理可能な音楽データを構築するのであれば，MEIのような記述の曖昧さを許容する構造は後のデータ処理や分析を煩わしいものにするだけで，そこから新しい知見がもたらされることは少ないだろう．最も，MIRの文脈でMEIを用いて作成されたデータを扱おうとする場合，前処理段階で元のデータが持っていた曖昧さは取り除かれ，分析に際してデータ記述の曖昧さによってもたらされた要素が結果が反映されることはないだろう．仮にそうでなければ，ノイズとして分析結果に悪影響を及ぼしたり，処理全体に必要な計算資源を無駄に消費したりするのが関の山である．</p>
<p>しかし，音楽情報の中からある一面をデータとして切り出して，それらをコンピュータで処理可能な表現に置き換えるという，現在のMIRにおいて必要とされている音楽情報の機械可読化は，MEIが掲げる音楽情報の機械可読化という目標とは異なる文脈に位置づけられることに注意する必要がある． MEIはRoland<span class="citation" data-cites="Roland2000">(<a href="references.html#ref-Roland2000" role="doc-biblioref">2000</a>)</span>がInternational Symposium on Music Information Retrieval<a href="#fn41" class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a>で行った発表に端を発する．その冒頭，Roland<span class="citation" data-cites="Roland2000">(<a href="references.html#ref-Roland2000" role="doc-biblioref">2000</a>)</span>は以下のように問いかけている．</p>
<blockquote class="blockquote">
<p>The most important word in the phrase “Music Information Retrieval” is “information”. Ponder why it’s not “Music Data Retrieval”. Information is not just data. Instead, information is communication, the exchanging of data and meta-data.</p>
<div style="text-align: right;">
<p><span class="citation" data-cites="Roland2000">(<a href="references.html#ref-Roland2000" role="doc-biblioref">Roland 2000, 1</a>)</span></p>
</div>
</blockquote>
<blockquote class="blockquote">
<p>音楽情報検索（Music Information Retrieval）という言葉の中で最も重要な言葉は「情報（information）」である．なぜ「音楽データ検索」ではないのだろうか．情報とは単なるデータではない．情報とはコミュニケーションであり，それはデータとメタデータの交換によってもたらされるのだ．</p>
<div style="text-align: right;">
<p>著者訳</p>
</div>
</blockquote>
<p>この発表においてRoland<span class="citation" data-cites="Roland2000">(<a href="references.html#ref-Roland2000" role="doc-biblioref">2000</a>)</span>は後のMEIにつながる音楽情報記述の方法を提示しつつ，音楽情報検索の分野が音楽“情報”ではなく，音楽“データ”の検索や処理に注力していることを暗に批判している．同様の批判は現在においてもかなりの部分が当てはまるが，そもそもMIRの発展自体が，研究の前提であると同時に大きな制約でもあるコンピュータという枠組みの中で，音楽をどのように扱うのかという問いに向き合ってきたことを考えれば，MIRが実質的に音楽データを対象とする研究として進展してきたことは，ある種の必然であったと言える．</p>
<section id="コンピュータ処理とmidiの親和性" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="コンピュータ処理とmidiの親和性">コンピュータ処理とMIDIの親和性</h4>
<p>楽譜とコンピュータという関係性については先に触れた通り1961年に始まったDARMSプロジェクトに1つの起点を見出すことができるが，コンピュータと音楽という文脈にまで広げるとさらに数年前にまで時代を遡ることができる． コンピュータで音楽情報を扱った最初期の取り組みとしては，コンピュータを用いた自動作曲の研究が挙げられる．第二次世界大戦の終戦を経て大学の非軍事研究で大型計算機が利用可能になると，それらを用いた自動作曲の研究が数多く取り組まれるようになる．イリノイ州立大学に設置されたILLIAC Iを用いて作曲された《イリアック組曲》(1957)はその代表例の1つであり，当作品はコンピュータを用いて作曲された初めての音楽作品とされる．コンピュータを使った情報処理タスクとして音楽を対象とするものは自動作曲以外にも次々と広がり，現在では自動採譜，演奏支援，音楽分析，楽曲推薦，光学楽譜認識，歌声合成など，幅広い領域を対象とする研究分野となっている．このようなタスクにおいて求められるのは，処理や分析の対象である音楽の構造的な記述をコンピュータで扱いやすい形式で，かつ大量に処理可能なデータとして表現することであり，そのために当時存在していた音楽情報記述形式の中からMIDIが広く採用されたのは自然な流れであったと言える．このような自動演奏から連なる音楽情報処理の分野において，コンピュータによる処理の対象となるシンボリックな音楽データのフォーマットとしては，汎用的かつ記述形式が明確に定められているMIDIが用いられることが圧倒的に多く，MIDIはMIRコミュニティにおけるデファクトスタンダードとしての地位を確かなものにしている．</p>
<p>また，一方ですでに国際的な研究コミュニティとして成立しているMIRにおける，研究インフラやデータ入手性の観点からもMIDIは圧倒的な優位性を誇っている．MIDIを扱うためのライブラリやフレームワークはすでに数多く整備されており，それらを使用すれば比較的簡単に研究や分析を始めることができる． また，民謡やクラシック音楽であればすでにMIDIを使用した音楽コーパスやデータセットが複数存在しており，楽譜制作ソフトウェアによるエクスポートが可能であることから，必要なデータの作成も容易である． このように，MIRの研究においてMIDIを用いることは，MIDIによって表現可能な音楽データの分析に取り組んでいる限りにおいては，研究の効率性やデータ入手性の観点からも合理的な選択であると言える．</p>
</section>
<section id="より柔軟な音楽情報記述体系の必要性" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="より柔軟な音楽情報記述体系の必要性">より柔軟な音楽情報記述体系の必要性</h4>
<p>一方で，このような合理的判断によってもたらされるMIDIの広範な使用は，MIRにおけるいくつかの問題における根本的な原因の一部となっており，それ故にRoland<span class="citation" data-cites="Roland2000">(<a href="references.html#ref-Roland2000" role="doc-biblioref">2000</a>)</span>は新しい音楽情報記述体系の必要性を論じた．MIRにおいてMIDIの使用がデファクトスタンダードになっているという事実は，MIRの射程がMIDIで表現可能な音楽データの内部に限定されており，その外側に広がるより多様な音楽情報を捨象しているということの裏返しでもある．そこで捨象されているデータの中には，Roland<span class="citation" data-cites="Roland2000">(<a href="references.html#ref-Roland2000" role="doc-biblioref">2000</a>)</span>が指摘するメタデータの記述も含まれている．MIDIが持つ楽器の操作に関する情報や楽譜上に表現された音楽的な構造そのものを対象とする限りにおいてであれば十分に有用なフォーマットであるが，それはデータを説明するデータであるメタデータの記述を十分に反映したものではなく，分析の結果を説明するだけのメタデータは含まれていない．Roland<span class="citation" data-cites="Roland2000">(<a href="references.html#ref-Roland2000" role="doc-biblioref">2000</a>)</span>の描いたメタデータとデータを介した情報のやり取りによって，MIDIのようなデータを用いた分析の結果に対して，メタデータの記述に基づいた洞察が与えられる．これは，定義された問題を解決する手段としてのデータ分析という一方向的なアプローチを超えて，データとその背後にある情報の関係性を考慮しつつ，データの分析から人間側が新しい洞察を得るという双方向的なアプローチを可能にする．</p>
<p>また，MIRが抱える根本的な問題として，研究や分析の対象となる音楽情報をコンピュータで処理可能な形式で表現する手段が必要不可欠であるという点が挙げられる．これは，かつて五線譜で書かれた情報をコンピュータで扱うために，その情報をどのように記述するかというデータモデリングやデータフォーマットに関する議論からスタートする必要があったのと同様である．現在では，文字情報にプレーンテキストが，西洋音楽の理論体系で記述可能な演奏情報にMIDIがそれぞれ対応しているが，それらは過去の議論によってもたらされた情報記述の枠組みであり，その外部に存在する情報を扱うためには新しい情報記述の方法論が求められる．これは，MIDIで記述することが難しい情報をMIRの文脈で扱うことのハードルを押し上げている．現実の音楽情報を見れば，MIDIの記述構造に起因する制約を受け入れることのできるデータは音楽情報全体のごく一部に過ぎないことは明らかである．このような問題はMIRにおける研究対象の偏りとしても表れている．</p>
<blockquote class="blockquote">
<p>The research carried out in MIR nowadays does not reflect the rich variety of the world’s music traditions. MIR studies are mostly focused on international mainstream popular music, Western art music and other styles mainly based on tonal principles and global consumption practices.</p>
<div style="text-align: right;">
<p><span class="citation" data-cites="InternationalSocietyforMusicInformationRetrieval2022">(<a href="references.html#ref-InternationalSocietyforMusicInformationRetrieval2022" role="doc-biblioref">International Society for Music Information Retrieval 2022</a>)</span></p>
</div>
</blockquote>
<blockquote class="blockquote">
<p>音楽情報学で取り組まれている研究は，国際的に主流なポピュラー音楽，西洋の芸術音楽，および調性の原理と世界の消費慣行に基づくその他の音楽様式に主たる焦点を当てており，世界の音楽的伝統が持つ豊かな多様性を反映していない．</p>
<div style="text-align: right;">
<p>著者訳</p>
</div>
</blockquote>
<p>International Society for Music Information Retrieval<span class="citation" data-cites="InternationalSocietyforMusicInformationRetrieval2022">(<a href="references.html#ref-InternationalSocietyforMusicInformationRetrieval2022" role="doc-biblioref">2022</a>)</span>が指摘しているのは，MIDIによる弊害というよりもMIRコミュニティ全体の偏重ではあるが，その一因としてMIDIのスタンダード化は少なからず影響を及ぼしているだろう．音楽文化の持つ真の多様性をMIRが反映するためにも，音楽情報そのものを記述の対象とするMEIのような取り組みは重要である．コンピュータでの処理は1つの可能性としつつも，従来のようなそれを前提にした画一的なフォーマットとは一線を画す柔軟な音楽情報記述の体系を構築することが，音楽データではなく音楽情報を対象とした研究のために必要であり，本研究もまたその一環として位置づけられる．</p>
</section>
</section>
</section>
<section id="小結" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="小結"><span class="header-section-number">5.3</span> 小結</h2>
<p>ここまで，西欧音楽を対象とする音楽情報の機械可読化に向けた取り組みを参照しつつ，音楽情報のデジタル化に向けた情報記述モデルについて整理してきた．音楽とコンピュータの近接性は音楽そのものが持つ性質に由来するものであり，現在に至るまで両者の間には様々な関係性が築かれてきている．音楽情報処理技術の研究開発という文脈では，自動作曲，光学楽譜認識，自動楽譜組版などがタスクとして掲げられてきたが，それは情報技術の開発であると同時に，人間による作曲行為や楽譜に対する認識をコンピュータ上で再現するという点において，人間の行動や認知に関する研究としても捉えることができる．そこで重要なのは音楽文化によってもたらされる情報の総体を正確にデータ化することではなく，各タスクの解決にとって重要な情報をコンピュータで処理しやすく，かつ大量に収集可能な形式で記述されたデータを採用することであった．</p>
<p>一方，人間が構築してきた音楽情報の記述や記録の手段としてコンピュータを導入するという文脈では，人間が実践してきた音楽記述のあり方を尊重しつつ，その多様な記述の総体をデジタルデータとして扱うための方法論が模索されてきた．もちろん，人間が生み出してきた多様な音楽情報の全容を網羅的に扱うことは難しく，実際には特定の音楽領域に焦点を当て，その特性を踏まえた情報記述モデルを構築する必要があるが，両者の間に存在する音楽情報記述に対する姿勢の違いは，MIRとして位置付けられる学問体系とコンピュータによる支援を伴う音楽学研究のそれぞれが対象とする根本的な問いの違いと捉えることもできるだろう．</p>
<p>人間の生み出してきた音楽文化である雅楽の記録と保存を担うデジタルデータの構築を目指す本研究は言うまでもなく後者の一部として位置づけられる取り組みである．</p>
<p>西洋音楽において作品の自律性が強調され，楽譜が作曲家の意図として扱われうるのとは対象的に，口伝による音楽伝承に支えられている雅楽の文化における楽譜はあくまでも演奏のための手控えであり，その情報は読み手の経験や知識を前提にして構成されている．また，西洋音楽において演奏が楽譜に書かれた作品の解釈として位置づけられるのとは異なり，雅楽における演奏は演奏者の身的な記憶によってもたらされるものであり，演奏を楽譜の解釈として位置づけることはできない．</p>
<p>雅楽の文化においてその中心として位置づけられるのは常に人間による音楽実践であり，相対的に見ればその他の資料はいずれも周辺的な存在に過ぎない．この事実は雅楽と雅楽譜という関係に限らず，人間による文化と所産という関係全般においても言えることであろう．<span class="math inline">\(\ref{chap:文化財保護と記録}\)</span>章で指摘したように，あらゆる有形の文化財の背後には人間による文化的活動が存在しており，有形文化財の保護や記録は，間接的ではあるが，その背後に存在する無形文化を保護し，記録することに他ならない．雅楽の記録にむけては，本章で整理した西洋音楽を中心に展開されてきた議論を踏まえながらも，雅楽の文化的特徴にあわせた情報記述モデルの構築が求められる．</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-Babbitt1965a" class="csl-entry" role="listitem">
Babbitt, Milton. 1965. <span>“The <span>Use</span> of <span>Computers</span> in <span>Musicological Research</span>.”</span> <em>Perspectives of New Music</em> 3 (2): 74–83. <a href="https://www.jstor.org/stable/832505">https://www.jstor.org/stable/832505</a>.
</div>
<div id="ref-Bent2001" class="csl-entry" role="listitem">
Bent, Ian D., David W. Hughes, Robert C. Provine, Richard Rastall, Anne Kilmer, David Hiley, Janka Szendrei, Thomas B. Payne, Margaret Bent, and Geoffrey Chew. 2001. <span>“Notation.”</span> <em>The New Grove Dictionary of Music and Musicians</em> 18: 73–189. <a href="https://doi.org/10.1093/gmo/9781561592630.article.20114">https://doi.org/10.1093/gmo/9781561592630.article.20114</a>.
</div>
<div id="ref-Berndt2018" class="csl-entry" role="listitem">
Berndt, Axel, Simon Waloschek, and Aristotelis Hadjakos. 2018. <span>“Meico: <span>A Converter Framework</span> for <span>Bridging</span> the <span>Gap</span> Between <span>Digital Music Editions</span> and Its <span>Applications</span>.”</span> In <em>Proceedings of the <span>Audio Mostly</span> 2018 on <span>Sound</span> in <span>Immersion</span> and <span>Emotion</span></em>. Wrexham, United Kingdom: Association for Computing Machinery. <a href="https://doi.org/10.1145/3243274.3243282">https://doi.org/10.1145/3243274.3243282</a>.
</div>
<div id="ref-Bloxam2001" class="csl-entry" role="listitem">
Bloxam, M. Jennifer. 2001. <span>“Cantus Firmus.”</span> Grove Music Online. <a href="https://doi.org/10.1093/gmo/9781561592630.article.04795">https://doi.org/10.1093/gmo/9781561592630.article.04795</a>.
</div>
<div id="ref-Boethius1867" class="csl-entry" role="listitem">
Boethius. 1867=2023. <em>Anicii <span>Manlii Torquati Severini Boetii De</span> Institutione Arithmetica Libri Duo, <span>De</span> Institutione Musica Libri Quinque.</em> Edited by Gottfried Friedlein. Lipsiae: in aedibus B. G. Teubneri. <a href="https://catalog.hathitrust.org/Record/000166661">https://catalog.hathitrust.org/Record/000166661</a>.
</div>
<div id="ref-Bowles1990" class="csl-entry" role="listitem">
Bowles, Garrett. 1990. <span>“Music <span>Software</span>.”</span> Edited by Robert Skinner. <em>Notes</em> 46 (3): 660–84. <a href="http://www.jstor.org/stable/941442">http://www.jstor.org/stable/941442</a>.
</div>
<div id="ref-CarlMariavonWeberGesamtausgabe2024" class="csl-entry" role="listitem">
Carl Maria von Weber Gesamtausgabe. 2024. <span>“<span>Projektbeschreibung</span>.”</span> Projektbeschreibung. <a href="https://www.weber-gesamtausgabe.de/de/Projekt/Projektbeschreibung.html">https://www.weber-gesamtausgabe.de/de/Projekt/Projektbeschreibung.html</a>.
</div>
<div id="ref-CenterforComputerAssistedResearchintheHumanities2010" class="csl-entry" role="listitem">
Center for Computer Assisted Research in the Humanities. 2010. <span>“<span>ABC</span> Plus.”</span> CCARH Wiki. <a href="https://wiki.ccarh.org/wiki/ABC_plus">https://wiki.ccarh.org/wiki/ABC_plus</a>.
</div>
<div id="ref-CenterforComputerAssistedResearchintheHumanities2015" class="csl-entry" role="listitem">
———. 2015. <span>“<span>IML-MIR</span>.”</span> CCARH Wiki. <a href="https://wiki.ccarh.org/wiki/IML-MIR">https://wiki.ccarh.org/wiki/IML-MIR</a>.
</div>
<div id="ref-CenterforComputerAssistedResearchintheHumanities2022" class="csl-entry" role="listitem">
———. 2022. <span>“<span>DARMS</span>.”</span> CCARH Wiki. <a href="https://wiki.ccarh.org/wiki/DARMS">https://wiki.ccarh.org/wiki/DARMS</a>.
</div>
<div id="ref-Chadabe1997" class="csl-entry" role="listitem">
Chadabe, Joel. 1997. <em>Electric Sound : The Past and Promise of Electronic Music</em>. Prentice Hall.
</div>
<div id="ref-Chadabe2001" class="csl-entry" role="listitem">
———. 2001. <span>“The <span>Electronic Century Part IV</span>: <span>The Seeds</span> of the <span>Future</span>.”</span> Electronic Musician. <a href="https://web.archive.org/web/20120928230435/http://www.emusician.com/gear/0769/the-electronic-century-part-iv-the-seeds-of-the-future/145415">https://web.archive.org/web/20120928230435/http://www.emusician.com/gear/0769/the-electronic-century-part-iv-the-seeds-of-the-future/145415</a>.
</div>
<div id="ref-DellEra2024" class="csl-entry" role="listitem">
Dell’Era, Greg. 2024. <span>“The <span>End</span> of <span>Finale</span>.”</span> Finale. <a href="https://www.finalemusic.com/blog/end-of-finale-new-journey-dorico-letter-from-president/">https://www.finalemusic.com/blog/end-of-finale-new-journey-dorico-letter-from-president/</a>.
</div>
<div id="ref-Dubowy2019" class="csl-entry" role="listitem">
Dubowy, Norbert. 2019. <span>“A <span>Music Edition</span> for the <span>Readership</span> of the 21st <span>Century</span>: <span>The Digital Interactive Mozart Edition</span>.”</span> In <em>American <span>Musicological Society</span> 2019</em>, 204–5. Boston: American Musicological Society. <a href="https://cdn.ymaws.com/www.amsmusicology.org/resource/resmgr/files/boston/2-ams_boston_2019_abstracts_.pdf">https://cdn.ymaws.com/www.amsmusicology.org/resource/resmgr/files/boston/2-ams_boston_2019_abstracts_.pdf</a>.
</div>
<div id="ref-Erickson1977" class="csl-entry" role="listitem">
Erickson, Raymond. 1977. <span>“Musicomp 76 and the <span>State</span> of <span>DARMS</span>.”</span> <em>College Music Symposium</em> 17 (1): 90–101. <a href="http://www.jstor.org/stable/40373862">http://www.jstor.org/stable/40373862</a>.
</div>
<div id="ref-Erickson1975" class="csl-entry" role="listitem">
Erickson, Raymond F. 1975. <span>“"<span>The Darms Project</span>": <span>A Status Report</span> on <span>JSTOR</span>.”</span> <em>Computers and the Humanities</em> 9 (6): 291–98. <a href="https://www.jstor.org/stable/30204239">https://www.jstor.org/stable/30204239</a>.
</div>
<div id="ref-Fujinaga2004" class="csl-entry" role="listitem">
Fujinaga, Ichiro, and Susan Forscher. 2004. <span>“Music.”</span> In <em>A Companion to Digital Humanities</em>, edited by Susan Schreibman, Raymond Siemens, and John Unsworth, 97–107. Malden, MA: Blackwell Pub.
</div>
<div id="ref-Gioia2023" class="csl-entry" role="listitem">
Gioia, Ted. 2023. <span>“How an <span>IBM Computer Learned</span> to <span>Sing</span> (1961).”</span> The Honest Broker. <a href="https://www.honest-broker.com/p/how-an-ibm-computer-learned-to-sing">https://www.honest-broker.com/p/how-an-ibm-computer-learned-to-sing</a>.
</div>
<div id="ref-Good2001" class="csl-entry" role="listitem">
Good, Michael. 2001. <span>“<span>MusicXML</span> for <span>Notation</span> and <span>Analysis</span>.”</span> In <em>The <span>Virtual Score</span>: <span>Representation</span>, <span>Retrieval</span>, <span>Restoration</span></em>, 12:113–24. Computing in <span>Musicology</span>. MIT Press. <a href="https://www.musicxml.com/publications/makemusic-recordare/notation-and-analysis/">https://www.musicxml.com/publications/makemusic-recordare/notation-and-analysis/</a>.
</div>
<div id="ref-Greg1950" class="csl-entry" role="listitem">
Greg, Walter Wilson. 1950=2016. <span>“The <span>Rationale</span> of <span>Copy-Text</span>.”</span> <em>Studies in Bibliography</em> 3 (1950=2016): 19–36. <a href="https://www.jstor.org/stable/40381874">https://www.jstor.org/stable/40381874</a>.
</div>
<div id="ref-InternationalSocietyforMusicInformationRetrieval2022" class="csl-entry" role="listitem">
International Society for Music Information Retrieval. 2022. <span>“Call for <span>Papers</span>.”</span> 23rd International Society for Music Information RetrievalInternational Society for Music Information Retrieval Conference 2023, Bengaluru. <a href="https://ismir2022.ismir.net/calls/cfp">https://ismir2022.ismir.net/calls/cfp</a>.
</div>
<div id="ref-Kassler1966" class="csl-entry" role="listitem">
Kassler, Michael. 1966. <span>“Toward <span>Musical Information Retrieval</span>.”</span> <em>Perspectives of New Music</em> 4 (2): 59–67. <a href="https://www.jstor.org/stable/832213">https://www.jstor.org/stable/832213</a>.
</div>
<div id="ref-Kepper2011" class="csl-entry" role="listitem">
Kepper, Johannes. 2011. <em><span>Musikedition im Zeichen neuer Medien. Historische Entwicklung und gegenw<span>ä</span>rtige Perspektiven musikalischer Gesamtausgaben</span></em>. BOOKS ON DEMAND. <a href="https://kups.ub.uni-koeln.de/6639/">https://kups.ub.uni-koeln.de/6639/</a>.
</div>
<div id="ref-Kirn2011" class="csl-entry" role="listitem">
Kirn, Peter. 2011. <em>Keyboard Presents the Evolution of Electronic Dance Music</em>. Backbeat Books.
</div>
<div id="ref-LibraryofCongress2024" class="csl-entry" role="listitem">
Library of Congress, and Network Development and MARC Standards Office. 2024. <span>“<span>MARC</span> 21 <span>Format</span> for <span>Bibliographic Data</span>.”</span> Library of Congress. <a href="https://www.loc.gov/marc/bibliographic/">https://www.loc.gov/marc/bibliographic/</a>.
</div>
<div id="ref-Lincoln1966" class="csl-entry" role="listitem">
Lincoln, Harry B. 1966. <span>“The <span>Computer Seminar</span> at <span>Binghamton</span>: <span>A Report</span>.”</span> <em>Notes</em> 23 (2): 236–40. <a href="https://www.jstor.org/stable/895404">https://www.jstor.org/stable/895404</a>.
</div>
<div id="ref-Maxwell1981" class="csl-entry" role="listitem">
Maxwell, John Turner. 1981. <span>“Mockingbird: An Interactive Composer’s Aid.”</span> Doctoral {{Thesis}}, Massachusetts Institute of Technology. <a href="https://dspace.mit.edu/handle/1721.1/15893">https://dspace.mit.edu/handle/1721.1/15893</a>.
</div>
<div id="ref-MIDIAssociation2015" class="csl-entry" role="listitem">
MIDI Association. 2015. <span>“<span>MIDI History</span>: <span>Chapter</span> 6-<span>MIDI Is Born</span> 1980-1983.”</span> The MIDI Association. <a href="https://midi.org/midi-history-chapter-6-midi-begins-1981-1983">https://midi.org/midi-history-chapter-6-midi-begins-1981-1983</a>.
</div>
<div id="ref-Morley1597" class="csl-entry" role="listitem">
Morley, Thomas. 1597. <em>A <span>Plain</span> and <span>Easy Introduction</span> to <span>Practical Music</span></em>. London: Peter Short. <a href="https://imslp.org/wiki/A_Plain_and_Easy_Introduction_to_Practical_Music_(Morley,_Thomas)">https://imslp.org/wiki/A_Plain_and_Easy_Introduction_to_Practical_Music_(Morley,_Thomas)</a>.
</div>
<div id="ref-Müller2021" class="csl-entry" role="listitem">
Müller, Meinard. 2021. <em>Fundamentals of Music Processing : Using Python and Jupyter Notebooks</em>. 2nd ed. Springer.
</div>
<div id="ref-MuseScore2024" class="csl-entry" role="listitem">
MuseScore. 2024. <span>“Soundfont, <span>MIDI</span> Velocity and Instruments.xml.”</span> Developers’ handbook. <a href="https://musescore.org/handbook/developers-handbook/references/instrumentsxml-documentation">https://musescore.org/handbook/developers-handbook/references/instrumentsxml-documentation</a>.
</div>
<div id="ref-MusicEncodingInitiative2024a" class="csl-entry" role="listitem">
Music Encoding Initiative. 2024. <span>“Introduction to <span>MEI</span>.”</span> Music Encoding Initiative. <a href="https://music-encoding.org/about/">https://music-encoding.org/about/</a>.
</div>
<div id="ref-MusicEncodingInitiative2023e" class="csl-entry" role="listitem">
Music Encoding Initiative, Johannes Kepper, and Perry D. Roland. 2023a. <span>“Customizing <span>MEI</span>.”</span> Text. Music Encoding Initiative Guidelines 5.0. <a href="https://music-encoding.org/guidelines/v5/content/introduction.html#meiCustomization">https://music-encoding.org/guidelines/v5/content/introduction.html#meiCustomization</a>.
</div>
<div id="ref-MusicEncodingInitiative2023a" class="csl-entry" role="listitem">
———. 2023b. <span>“Definitions and <span>Parameters</span>.”</span> Music Encoding Initiative Guidelines 5.0. <a href="https://music-encoding.org/guidelines/v5/content/introduction.html#definitionsAndParameters">https://music-encoding.org/guidelines/v5/content/introduction.html#definitionsAndParameters</a>.
</div>
<div id="ref-MusicEncodingInitiative2023b" class="csl-entry" role="listitem">
———. 2023c. <span>“General <span>Principles</span>.”</span> Music Encoding Initiative Guidelines 5.0. <a href="https://music-encoding.org/guidelines/v5/content/introduction.html#generalPrinciples">https://music-encoding.org/guidelines/v5/content/introduction.html#generalPrinciples</a>.
</div>
<div id="ref-MusicEncodingInitiative2023d" class="csl-entry" role="listitem">
———. 2023d. <span>“Musical <span>Domains</span>.”</span> Text. Music Encoding Initiative Guidelines 5.0. <a href="https://music-encoding.org/guidelines/v5/content/introduction.html#musicalDomains">https://music-encoding.org/guidelines/v5/content/introduction.html#musicalDomains</a>.
</div>
<div id="ref-Newcomb1991a" class="csl-entry" role="listitem">
Newcomb, Steven R. 1991. <span>“Standards: <span>Standard Music Description Language</span> Complies with Hypermedia Standard.”</span> <em>Computer</em> 24 (7): 76–79. <a href="https://doi.org/10.1109/2.84842">https://doi.org/10.1109/2.84842</a>.
</div>
<div id="ref-OpenScore2018a" class="csl-entry" role="listitem">
OpenScore. 2018. <span>“<span>OpenScore</span>: <span>One Year On</span>.”</span> OpenScore. <a href="https://openscore.cc/blog/2018/8/20/openscore-one-year-on">https://openscore.cc/blog/2018/8/20/openscore-one-year-on</a>.
</div>
<div id="ref-Patrick1975" class="csl-entry" role="listitem">
Patrick, P. Howard, and Patricia Friedman. 1975. <span>“Computer Printing of <span>Braille</span> Music Using the <span>IML-MIR</span> System.”</span> <em>Computers and the Humanities</em> 9 (3): 115–21. <a href="https://doi.org/10.1007/BF02404294">https://doi.org/10.1007/BF02404294</a>.
</div>
<div id="ref-Plaksin2019" class="csl-entry" role="listitem">
Plaksin, Anna, and Jacob Olley. 2019. <span>“Creating an <span>Encoding Workflow</span> for a <span>Critical Edition</span> of <span>Ottoman Music Manuscripts</span>: <span>Challenges</span> and <span>Solutions</span>.”</span> In <em>Music <span>Encoding Conference Proceedings</span> 2015, 2016 and 2017</em>, edited by Giuliani Di Bacco, Johannes Kepper, and Perry D. Roland, 119–30. Music Encoding Conference. <a href="https://www.academia.edu/41059516/Creating_an_Encoding_Workflow_for_a_Critical_Edition_of_Ottoman_Music_Manuscripts_Challenges_and_Solutions">https://www.academia.edu/41059516/Creating_an_Encoding_Workflow_for_a_Critical_Edition_of_Ottoman_Music_Manuscripts_Challenges_and_Solutions</a>.
</div>
<div id="ref-Rettinghaus2023" class="csl-entry" role="listitem">
Rettinghaus, Klaus. 2023. <span>“Comparison of <span>MusicXML</span> Export Capabilities of Different Scorewriters.”</span> In <em><span>“<span>Encoding Cultures</span>”</span> <span>Joint MEC TEI</span> Conference 2023 – <span>Book</span> of <span>Abstracts</span></em>. En. Zenodo. <a href="https://doi.org/10.17613/5qq5-jn04">https://doi.org/10.17613/5qq5-jn04</a>.
</div>
<div id="ref-Roland2000" class="csl-entry" role="listitem">
Roland, Perry. 2000. <span>“<span>XML4MIR</span>: <span>Extensible Markup Language</span> for <span>Music Information Retrieval</span>.”</span> In <em>International <span>Symposium</span> on <span>Music Information Retrieval</span></em>, 1–9. Plymouth, Massachusetts. <a href="https://ismir2000.ismir.net/papers/roland_paper.pdf">https://ismir2000.ismir.net/papers/roland_paper.pdf</a>.
</div>
<div id="ref-Rowe1993" class="csl-entry" role="listitem">
Rowe, Robert D. 1993. <em>Interactive Music Systems : Machine Listening and Composing</em>. MIT Press.
</div>
<div id="ref-SanAndreasEnterprises2003" class="csl-entry" role="listitem">
San Andreas Enterprises. 2003. <span>“The <span>SCORE Music Publishing System</span>.”</span> San Andreas Press. <a href="https://web.archive.org/web/20030401151110/http://www.scoremus.com/score.html">https://web.archive.org/web/20030401151110/http://www.scoremus.com/score.html</a>.
</div>
<div id="ref-Schreibman2024" class="csl-entry" role="listitem">
Schreibman, Susan, and ISGMLUG. 2024. <span>“A <span>Short History</span> of <span>SGML</span>.”</span> ISGMLUG. <a href="https://isgmlug.org/a-short-history-of-sgml/">https://isgmlug.org/a-short-history-of-sgml/</a>.
</div>
<div id="ref-Sloan1995" class="csl-entry" role="listitem">
Sloan, Donald. 1995. <span>“A <span>Brief History</span> of <span>HyTime</span> and <span>SMDL</span>.”</span> An Introductory Description of HyTime and SMDL. <a href="https://www.lim.di.unimi.it/IEEE/SLOAN/BRIEF.HTM">https://www.lim.di.unimi.it/IEEE/SLOAN/BRIEF.HTM</a>.
</div>
<div id="ref-Sloan1997" class="csl-entry" role="listitem">
———. 1997. <span>“<span>HyTime</span> and <span>Standard Music Description Language</span>: <span>A Document-Description Approach</span>.”</span> In <em>Beyond <span>MIDI</span>: <span>The Handbook</span> of <span>Musical Codes</span></em>, edited by Eleanor Selfridge-Field, 469–90. MIT Press.
</div>
<div id="ref-Tschiggfrie2020" class="csl-entry" role="listitem">
Tschiggfrie, Jon. 2020. <span>“When <span>Sheet Music Goes Digital</span>: <span>The Life</span> and <span>Times</span> of <span>MusicXML</span>.”</span> Finale. <a href="https://www.finalemusic.com/blog/when-sheet-music-goes-digital-the-life-and-times-of-musicxml/">https://www.finalemusic.com/blog/when-sheet-music-goes-digital-the-life-and-times-of-musicxml/</a>.
</div>
<div id="ref-Walder2016" class="csl-entry" role="listitem">
Walder, Christian. 2016. <span>“Symbolic <span>Music Data Version</span> 1.0.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.1606.02542">https://doi.org/10.48550/arXiv.1606.02542</a>.
</div>
<div id="ref-Walshaw2024" class="csl-entry" role="listitem">
Walshaw, Chris. 2024. <span>“About Abc Notation.”</span> abcnotation.com. <a href="https://abcnotation.com/about">https://abcnotation.com/about</a>.
</div>
<div id="ref-Zeng2021" class="csl-entry" role="listitem">
Zeng, Mingliang, Xu Tan, Rui Wang, Zeqian Ju, Tao Qin, and Tie-Yan Liu. 2021. <span>“<span>MusicBERT</span>: <span>Symbolic Music Understanding</span> with <span>Large-Scale Pre-Training</span>.”</span> arXiv. <a href="https://doi.org/10.48550/arXiv.2106.05630">https://doi.org/10.48550/arXiv.2106.05630</a>.
</div>
<div id="ref-デジタル大辞泉2024a" class="csl-entry" role="listitem">
デジタル大辞泉. 2024. <span>“<span>オープン-フォーマット【open format】</span>.”</span> JapanKnowledge. <a href="https://japanknowledge.com/lib/display/?lid=2001029960744">https://japanknowledge.com/lib/display/?lid=2001029960744</a>.
</div>
<div id="ref-今郷1992" class="csl-entry" role="listitem">
今郷詔. 1992. <span>“<span>HyTime——文書の拡張としてのハイパーメディア記述言語</span>.”</span> <em>情報処理学会研究報告データベースシステム（DBS）</em> 1992 (86(1992-DBS-091)): 47–56. <a href="https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&amp;page_id=13&amp;block_id=8&amp;item_id=20416&amp;item_no=1">https://ipsj.ixsq.nii.ac.jp/ej/index.php?active_action=repository_view_main_item_detail&amp;page_id=13&amp;block_id=8&amp;item_id=20416&amp;item_no=1</a>.
</div>
<div id="ref-加藤1999" class="csl-entry" role="listitem">
加藤博之, and 水野升裕. 1999. <span>“情報の電子化技術－入門から応用まで.”</span> <em>情報管理</em> 42 (9): 777–90. <a href="https://doi.org/10.1241/johokanri.42.777">https://doi.org/10.1241/johokanri.42.777</a>.
</div>
<div id="ref-北口2019" class="csl-entry" role="listitem">
北口二朗. 2019. <span>“<span>電子楽器の技術発展の系統化調査</span>.”</span> In <em><span>国立科学博物館技術の系統化調査報告</span></em>, 26:1–89. 国立科学博物館.
</div>
<div id="ref-大愛2021" class="csl-entry" role="listitem">
大愛崇晴. 2021. <em><span>16・17世紀の数学的音楽理論——音楽の数量化と感性的判断をめぐって</span></em>. 晃洋書房.
</div>
<div id="ref-布川2024" class="csl-entry" role="listitem">
布川角左衛門. 2024. <span>“<span>奥付</span>.”</span> 改訂新版世界大百科事典. <a href="https://kotobank.jp/word/奥付-451769#w-1151525">https://kotobank.jp/word/奥付-451769#w-1151525</a>.
</div>
<div id="ref-田中2015" class="csl-entry" role="listitem">
田中健次. 2015. <span>“<span>電子楽器の100年</span>.”</span> In <em><span>浜松市楽器博物館総合案内</span></em>, 232–37. 浜松市楽器博物館.
</div>
<div id="ref-高野2024" class="csl-entry" role="listitem">
高野彰. 2024. <span>“<span>写本</span>.”</span> 日本大百科全書. <a href="https://japanknowledge.com/lib/display/?kw=incipit&amp;lid=1001000111612">https://japanknowledge.com/lib/display/?kw=incipit&amp;lid=1001000111612</a>.
</div>
</div>
</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>このような，協和音を中心とする音程関係を数学的な手段によって考察する理論的試みである「数学的音楽理論」は聴覚的な判断の根拠を数学的合理性に求めたが，客観的に定量化可能な量に基づいた数学的知見を通して，人間の認知や感性といった感覚的知覚を十分に汲みつくすことはできないという限界が指摘されている<span class="citation" data-cites="大愛2021">(<a href="references.html#ref-大愛2021" role="doc-biblioref">大愛 2021</a>)</span>．ただし，古代ギリシャのような数比に基づく音楽へのアプローチが必ずしも人間の感覚に対してアプローチしていたとは限らないということには注意する必要があるだろう．客観的に観察可能な事実とそれに対する人間の感覚が異なるということは決して珍しいことではない．数比を通して追求した調和はあくまでも数学的な調和と人間の感覚的な調和が連関するという前提に立っており，その前提を超えた議論には到達できないという限界は確かに存在するかもしれないが，そのアプローチ自体が数学的な興味からもたらされているのだとすれば，大愛<span class="citation" data-cites="大愛2021">(<a href="references.html#ref-大愛2021" role="doc-biblioref">2021</a>)</span>の指摘する限界はそもそもギリシャにおける音楽研究の射程に存在しない問いであると考えることもできる．<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>日米3社による共同開発として進行したが，実際にが採用した構造の80–90%はRolandが開発していたDigital Control Busをベースとしており，通信ケーブルも同じくRolandが開発したDIN Syncで用いられていた5ピンのDINコネクタが採用された<span class="citation" data-cites="Kirn2011">(<a href="references.html#ref-Kirn2011" role="doc-biblioref">Kirn 2011</a>)</span>．<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>アメリカの音楽業界団体であるNAMMがカルフォルニア州アナハイムで毎年行っている見本市．<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>2つの楽器をMIDIケーブルで接続し，片方の鍵盤を弾くと，もう一方のシンセサイザーの音源が鳴る，という極めてシンプルな実験だった<span class="citation" data-cites="北口2019">(<a href="references.html#ref-北口2019" role="doc-biblioref">北口 2019, 47</a>)</span>．<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>ここで言う“デジタル”とは，デジタル“音源”の意味である．田中<span class="citation" data-cites="田中2015">(<a href="references.html#ref-田中2015" role="doc-biblioref">2015, 235</a>)</span>は1945年から1980年までをアナログ回路を用いた波形合成が主流なアナログの時代，1980年から1995年をデジタルの時代と捉えている．MIDIがリリースされた1980年代初頭はまさに電子楽器がアナログからデジタルへと向かう変遷期だったのである．<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>デジタル楽器が鍵盤というコンセプトに依存することは重大な制約だと感じるユーザーも一部にはいたが，多くの人にとってはMIDIによって実現した標準化による利点の方がはるかに大きかった<span class="citation" data-cites="Chadabe2001">(<a href="references.html#ref-Chadabe2001" role="doc-biblioref">Chadabe 2001</a>)</span>．<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>演奏情報については高い精度を持つMIDIだが，MIDIがあるからといってコンピュータが元の演奏をシミュレーションできる訳ではない．楽譜がすべての音楽情報を記述し得ないように，符号化には情報の欠落が常に生じる<span class="citation" data-cites="Rowe1993">(<a href="references.html#ref-Rowe1993" role="doc-biblioref">Rowe 1993, 120</a>)</span>．例えば，MIDIは演奏というインプットを記録するが，それによって楽器がどのように鳴ったかというアウトプットの部分には立ち入らない．したがって，特にアコースティック楽器の操作を対象とする場合，実際の演奏が持つ音色やその変化に関する情報をかなり無視した記述にならざるをえない．<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>MIDIは大きく分けるとデータの基本情報を記述するヘッダー･チャンクと旋律に関する情報を記述するトラックチャンクに分けられる．<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>鍵盤の押す / 離すをそれぞれ <code>Note on</code> / <code>Note off</code>として指示する．<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>C4を60として各鍵盤に当てはめられた0から127までの数字で指定する．ピアノの88鍵盤は21から108に該当する．<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>楽器同士のリアルタイムな同期のために作られたMIDIに八分音符や四分音符といった西洋音楽記譜法に基づいた基準は存在せず，イベントの経過時間はSMPTEタイムコードもしくはTickが用いられる．MIDIにおける時間の最小単位であるTickはヘッダー･チャンク内で指定される分解能(4分音符の長さを何Tickに分割するか，単位はPulses per quarter note: PPQ)によって定義される．PPQN=480tickの場合，全音符は1920tick，八分音符は240tickになるが，スタッカートなどを再現する場合であれば八分音符を240tickよりも短く指定することもある．<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>直前のイベントに対してどれだけの時間間隔を取るか(どれだけ遅れるか)をSMPTEタイムコードもしくはTickで指定する．0の場合は直前のイベントと同時，すなわち和音になる．<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>1を最小，127を最大とする127段階で指示する．シンセサイザーは打鍵の速さを演奏の強さとして認識させているが，速度センサーを搭載していない機材の場合はメゾフォルテ付近に相当する64が指定される．0は無音であり，Event Typeの<code>Note off</code>と同様イベントの終わりを指示するために用いられる．<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>Melvin Ferentzがプロジェクト全体にどのように関わっていたのかは明らかでないが，1966年にニューヨーク大学で2週間に渡って行われたコンピュータを使用した音楽研究に関連したセミナー兼ワークショップの報告に講師として名前が挙げられている<span class="citation" data-cites="Lincoln1966">(<a href="references.html#ref-Lincoln1966" role="doc-biblioref">Lincoln 1966</a>)</span>．また，Stefan Bauer-MengelbergとMelvin Ferentzは十二音技法を用いて作曲されたAlban Maria Johannes Berg（1885–1935）による（抒情組曲）に使用された音列をIBM 7094を用いて分析しており，楽譜のデジタル化以外の分野でも協力関係にあった．ちなみに，IBM 7094は世界で初めて“歌った”コンピュータとしても知られている<span class="citation" data-cites="Gioia2023">(<a href="references.html#ref-Gioia2023" role="doc-biblioref">Gioia 2023</a>)</span>．<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>括弧内の等幅フォントで示された<code>n</code>は任意の自然数を示す．<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>1963年に開設されたスタンフォード大学における初期のコンピュータサイエンスに関する研究機関．1980年には一度独立研究所としては閉鎖されてコンピュータサイエンス学部に統合されたが2004年に復活している．SAILの初期メンバーの写真をアーカイブしたサイトには，Leland Smithをはじめ，を開発したDonald Knuth，コピー&amp;ペーストのコマンドを生み出したLarry Tesler，FM音源の合成アルゴリズムを発見したJohn Chowning，Sequential Circuitsの創業者であるDave Smithなど，初期コンピュータサイエンスの重要人物が名を連ねている．Photographs of early Stanford AI-Lab People，(Retrieved 2024-11-01, <a href="http://infolab.stanford.edu/pub/voy/museum/pictures/AIlab/list.html">http://infolab.stanford.edu/pub/voy/museum/pictures/AIlab/list.html</a>)．<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>ユーザーインターフェイスの一類型であり，画面上に表示される通りの印刷物が出力されることを保証する編集作業環境のこと．Microsoft Wordのようなワープロソフト，MuseScoreをはじめとする楽譜制作ソフトウェア，Adobe Illustratorなどのイメージ編集ソフトウェアなど，印刷物の作成を目的としたサービスで積極的に採用されてきたが，Webサイトをノーコードで構築する場合など，最終的なアウトプットが印刷物ではない場合にも用いられている．対応する概念であるWYSIWYMは，テキストの内容と見た目が分離された構造化テキストを示す用語であり，ユーザーは最終的な見た目ではなく，そのテキストが何を意味しているかを指定しながらテキストを構造的に構築していくことが求められる．<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>ゴルトベルク変奏曲および平均律クラヴィーア曲集 第1巻の高品質なデジタル楽譜と演奏音源をCreative Commons 0ライセンスで公開したプロジェクト．イシザカ キミコによる演奏とMuseScoreを用いてデジタル翻刻されたデジタル楽譜が利用制限なく無料で公開されている．プロジェクトの資金はKickStarterを経由してクラウドファンディングで賄われた．<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p>Avid Technology社が開発・販売を手掛けるSibeliusとともに世界の二大楽譜制作ソフトウェアの一角を担ってきたMakeMusic社のFinaleがサービス終了を宣言したことによって，MusicXMLをめぐるファイル互換性の問題は現在非常に注目されている<span class="citation" data-cites="DellEra2024">(<a href="references.html#ref-DellEra2024" role="doc-biblioref">Dell’Era 2024</a>)</span>．Finaleは移行先のソフトウェアとしてSteinberg社のDoricoを推奨しているが，FinaleでエクスポートしたMusicXMLファイルをDoricoで読み込むと，一部の記号が正しく表示されず，Doricoを使用して外観を調整する必要がある．もちろん，世界中のFinaleユーザーが構築してきた大量の楽譜がMusicXMLによって救出され，新しいソフトウェアで再利用されるであろうことは朗報ではあるが，同時にMusicXMLによるファイル共有の限界が強調されることにもなった．<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>特に楽譜再生機能の強化は各楽譜制作ソフトウェアが競って開発に取り組んでいる機能の1つである．搭載している音源ソフトウェアのアップデートなどによってより生の楽器に近いリアルな音質を追求するとともに，フレットノイズ，チョーイング，ビブラートといった演奏表現の再現の実現が目指されており，中には楽譜制作ソフトウェア上で作曲（楽譜の作成）から簡易的なデモ音源の作成までのワークフローを完結させることができる水準に到達しているソフトウェアも存在する．<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p>incipitはラテン語で「ここから始まる」の意味．書籍の場合，タイトルページを挿入する習慣は刊本の登場によってもたらされた工夫であり，それ以前の写本では本文を“incipit”（ここから始まる）で開始し，“explicit”（ここで終わる）で本文を結び，続いて著者，書写日，写字生を記したコロフォンを挿入する日本の奥付によく似た習慣があった<span class="citation" data-cites="高野2024">(<a href="references.html#ref-高野2024" role="doc-biblioref">高野 2024</a>)</span>．コロフォンの習慣は1520年ごろから巻頭に移り現在のようなtitle versoが形成されていった<span class="citation" data-cites="布川2024">(<a href="references.html#ref-布川2024" role="doc-biblioref">布川 2024</a>)</span>．<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p>もちろん，MusicXMLでは直接指定されていないベロシティに関する記述など，MIDIを生成する上で補完された情報も存在するため，生成されたMIDIが“演奏データ”だからと言って，そこに存在するベロシティの記述から演奏表現における強弱を対象とした分析を行うことはできない．<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23"><p>より正確にいうと，初期のMIDI規格そのものには音色の定義は含まれておらず，音源ごとに楽器を指定するための番号が異なっていた．従って，A社の音源で再生するとピアノの音で演奏されるMIDIデータをB社の音源で再生するとギターの音色で再生されるといったことがしばしば発生していた．音色の指定については，のちに制定されたGMを使用することで音源間の互換性を維持している．<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24"><p>法的な制約なしに自由に利用できるファイルフォーマット．フォーマットの仕様が公開されているため，ファイルを作成したソフトウェア製品の製造元がなくなっても，長期的にわたって利用することができる<span class="citation" data-cites="デジタル大辞泉2024a">(<a href="references.html#ref-デジタル大辞泉2024a" role="doc-biblioref">デジタル大辞泉 2024</a>)</span>．<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25"><p>バロックから現代までのヨーロッパの音楽劇の代表的な作品の校訂版を制作することを目指してスイス科学芸術アカデミーが主宰するプロジェクト．9つの作品が取り上げられ，「原文と翻訳」，「演奏法と解釈」など，特定の課題を対象とした複数のモジュールで構成されることが予定されている．楽譜は印刷楽譜として，出典を含む評論レポートはデジタルプラットフォーム上で公開される，ハイブリッド・エディションとして提供される．（<a href="https://www.adwmainz.de/?id=1893&amp;L=0">https://www.adwmainz.de/?id=1893&amp;L=0</a>，2024-10-13取得）<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26"><p>後期ロマン派の作曲家マックス・レーガーの学術校訂版編纂プロジェクト．楽譜は印刷楽譜としてCarus-Verlag社から出版され，レポートはデジタルプラットフォーム上で公開されるハイブリッド・エディションの形式を採用している．（<a href="https://www.reger-werkausgabe.de/">https://www.reger-werkausgabe.de/</a>，2024-10-13取得）<a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27"><p>カール・マリア・フォン・ウェーバー (Carl Maria von Weber, 1786–1826)の全作曲，書簡，日記，著作を学術的に批評された完全版として出版することを目的としたプロジェクト．（<a href="https://www.weber-gesamtausgabe.de/">https://www.weber-gesamtausgabe.de/</a>，2024-10-13取得）．ウェーバー没後200年にあたる2026年までの完成を目指しているが，プロジェクトとして収集した書簡，日記，著作，楽譜などの資料の一部はすでにオンラインで公開している．学術批評のプロセスではしばしば新しい問題や検討事項が見つかり，一般的にはそれらがすべて解決，整理された状態で出版のプロセスに移るため，WeGAが採用した編集状態にある資料の先行公開は異例な措置である．これについて，Carl Maria von Weber Gesamtausgabe<span class="citation" data-cites="CarlMariavonWeberGesamtausgabe2024">(<a href="references.html#ref-CarlMariavonWeberGesamtausgabe2024" role="doc-biblioref">2024</a>)</span>は「完璧を追求しすぎたことで特定の編集結果が一般に公開されず，さらなる研究のための貴重な情報が失われてしまうことがあまりにも多い」と指摘しており，完成を待たずに資料の一部を公開することに伴って「自身の弱点や無知をさらけ出すこと」や「不完全な成果や不整合性をさらす」リスク以上に，プロジェクト外部の研究者や一般のオーディエンスにプロジェクトの構築しつつある情報を公開することでプロジェクトの透明性を高め，オープンで協力的なコラボレーションを生み出すことを優先した，と述べている．<a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn28"><p>国際モーツァルテウム財団が編集したNMA（新モーツァルト全集）に含まれる楽譜と批判校訂報告をすべて公開しているサイト．（ <a href="https://dme.mozarteum.at/DME/nma/start.php">https://dme.mozarteum.at/DME/nma/start.php</a>，2024-10-16取得）<a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn29"><p>例えば，ドイツでは発行から25年間，イタリアでは20年間の保護期間が与えられている．また，欧州委員会の「著作権保護期限指令 (2006/116/EC)」の第5条は，パブリックドメインとなった著作物の校訂版および科学的出版物の保護期間を出版物が最初に合法的に公表された時点から最長30年とすることが定められており，国内での法整備が行われればEU各国は国内で発行される原典版や学術校訂版楽譜について最大30年の保護期間を設けることができる．（European Union，<em>Directive 2006/116/EC of the European Parliament and of the Council of 12 December 2006 on the term of protection of copyright and certain related rights，</em> EUR-Lex，Retrieved 2024-10-15, <a href="http://data.europa.eu/eli/dir/2006/116/oj">http://data.europa.eu/eli/dir/2006/116/oj</a>）<a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn30"><p>国際モーツァルテム財団がDMEの中核プロジェクトとして展開している新しいデジタル学術校訂版の編纂プロジェクト．MEIを採用し，NMAが培ってきた学術的成果を尊重しつつも，内容，外見ともにNMAとは異なる新しい原典版として打ち出されている<span class="citation" data-cites="Dubowy2019">(<a href="references.html#ref-Dubowy2019" role="doc-biblioref">Dubowy 2019</a>)</span>．<a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn31"><p>Kassler<span class="citation" data-cites="Kassler1966">(<a href="references.html#ref-Kassler1966" role="doc-biblioref">1966</a>)</span>はMIRという言葉が初めて使用された論文であるとされている．Kasslerは，Arthur Mendel，Tobias Robinson，J. Horace Patrickらとともに，中世の単旋律音楽の符号化を対象としたIML-MIRを開発した<span class="citation" data-cites="CenterforComputerAssistedResearchintheHumanities2015">(<a href="references.html#ref-CenterforComputerAssistedResearchintheHumanities2015" role="doc-biblioref">Center for Computer Assisted Research in the Humanities 2015</a>)</span>．IML-MIRが技術標準として広まることはなかったが，その後点字楽譜のタイプセット言語として拡張されるなど，1つの音楽情報から複数のアウトプットを得ることが実践された<span class="citation" data-cites="Patrick1975">(<a href="references.html#ref-Patrick1975" role="doc-biblioref">Patrick and Friedman 1975</a>)</span>．<a href="#fnref31" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn32"><p>現代音楽のような複雑な記譜法にも対応することを目指した音楽編集ソフトウェア． 作曲家の創作活動を支援するシステムという側面が強く，現在の楽譜制作ソフトウェアやDAWとは異なる役割をもったソフトウェアとしてデザインされている．また，作曲家の支援に焦点をあてることで，ソフトウェア開発を通じて楽譜ソフトウェアに対する作曲家のニーズを明らかにしようとしている．Xerox PARCが開発したプログラム言語であるMesaで開発され，ヤマハCP-30の76鍵シンセサイザーを接続したXerox DaybreakシリーズのDorado上で動作した<span class="citation" data-cites="Maxwell1981">(<a href="references.html#ref-Maxwell1981" role="doc-biblioref">Maxwell 1981</a>)</span>．<a href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn33"><p>行政，軍事，科学研究，航空産業，商業出版など，長期的なメンテナンスが必要な大規模文書を,その内容とレイアウトを分離して記述，管理することを目的に開発されたマークアップ言語，1986年にISO規格として制定されている．文書の構造に使用するスキーマ（DTD）が非常に柔軟に運用できる一方，SGMLの規格に忠実な処理系の実装は複雑でメンテナンスに高いコストがかかったため，SGMLの普及はそのコストに見合う程度の規模を持った組織やプロジェクトに限定されていた<span class="citation" data-cites="加藤1999 Schreibman2024">(<a href="references.html#ref-加藤1999" role="doc-biblioref">加藤 and 水野 1999</a>; <a href="references.html#ref-Schreibman2024" role="doc-biblioref">Schreibman and ISGMLUG 2024</a>)</span>．本論文が使用するXMLはSGMLの後継にあたり，SGMLの基本的な仕組みを引き継ぎつつ，DTDの定義がない状態でも利用に支障がないように簡素化されている．<a href="#fnref33" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn34"><p>SGMLを拡張することで，文書内及び文書間の相互結合や他の情報オブジェクトとの相互結合を実現し，時間的及び空間的なマルチメディア情報の記述を実現した標準規格（ISO/IEC 10744）．SGML，HyTime，SMDLはいずれもIBM社のCharles F. Goldfarbが主導するANSI内の研究グループ（ANSI X3V1.8M）によって開発された<span class="citation" data-cites="Sloan1995">(<a href="references.html#ref-Sloan1995" role="doc-biblioref">Sloan 1995</a>)</span>．<a href="#fnref34" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn35"><p>Standard Music Description Language，(Retrieved 2024-11-01，<a href="https://www.lim.di.unimi.it/IEEE/SMDL/INDEX.HTM">https://www.lim.di.unimi.it/IEEE/SMDL/INDEX.HTM</a>)<a href="#fnref35" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn36"><p>マニュアル文書には“任意の数”とあるが，音楽作品の存在と論理領域の存在が切り離された状態，すなわちcantus要素を持たず，論理領域が空であるという状態は考えられないため，実質的には1つ以上のcantus要素を持つと解釈できる．cantusという言葉は，「多声性を有する音楽の様式における音楽時間進行の核として機能する基本旋律」<span class="citation" data-cites="Bloxam2001">(<a href="references.html#ref-Bloxam2001" role="doc-biblioref">Bloxam 2001</a>)</span>を指すcantus firmusに由来している．<a href="#fnref36" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn37"><p>SMDLの定義では「あるcantusから別の有限座標空間へのイベントの「プロジェクション」を制御する」とある．今郷<span class="citation" data-cites="今郷1992">(<a href="references.html#ref-今郷1992" role="doc-biblioref">1992</a>)</span>がバトン要素を「指揮棒要素」と訳出しているように，仮想時間軸しか持たないcantusをどのように実時間軸へと射影するかを司る指揮者の役割を担っていると理解するとわかりやすいだろう．<a href="#fnref37" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn38"><p>アクセント記号を持たない文字コードでテキストを構成することが求められるような環境でアクセント記号付きの文字を表現する場合など，テキストにおいても使用されている文字を直接扱えないケースが存在しないわけではない．このような場合には，いくつかの記号を組み合わせて対象のアクセント付き文字を表現する翻字によって対応される事が多い．例えば，plainTeX で特殊文字を使用する場合， ó：<code>\'o</code>， ò：<code>\`o</code>， ô：<code>\^o</code>， ö：<code>\"o</code>， õ：<code>\~o</code>， ō：<code>\=o</code>， ż：<code>\.z</code>， ç：<code>\c{c}</code>， š：<code>\v{s}</code>， ă：<code>\u{a}</code>， ů：<code>\r{u}</code>， ő：<code>\H{o}</code>， ọ：<code>\d{o}</code>， b̲：<code>\b{b}</code>， T͡S：<code>\t{ts}</code>， Ø：<code>\O</code>， Ł：<code>\L</code>， ł：<code>\l</code>， Å：<code>\AA</code>， ı：<code>\i</code>， ȷ：<code>\j</code>， のような翻字表記が用いられる．もちろん，自然言語に存在する文字記号はさらに多く，同様の方法を使ってさらに多くの言語が用いる特殊文字に対応したパッケージが開発されている．<a href="#fnref38" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn39"><p>Walshaw, Chris，abcnotation.com（Retrieved 2024-10-01，<a href="https://abcnotation.com/">https://abcnotation.com/</a>）<a href="#fnref39" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn40"><p>このような特殊なケースは演奏者研究において発生する可能性がある．演奏者が用いた楽譜と，その楽譜を用いて演奏された録音や映像との関係性を記述する場合，両者の関係性は直接的な対応関係を見出されうる．しかし，このような場合であって，ある演奏者が同一楽曲に対して2つの異なる楽譜を用いている場合や，特定の楽譜との関係性を見いだしにくい演奏を扱う必要がある場合など，抽象的な楽譜の単位を定義することで関心の分離がより明確になり，それぞれの関係性が描き出しやすい可能性もある．<a href="#fnref40" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn41"><p>現在のISMIR<a href="#fnref41" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter3.html" class="pagination-link" aria-label="雅楽とその記録としての譜">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">雅楽とその記録としての譜</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter5.html" class="pagination-link" aria-label="XMLを用いた雅楽譜の翻刻">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">XMLを用いた雅楽譜の翻刻</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>